---
layout: post
title:  "(SPADE) Synthetic Image Synthesis with Spatially-Adaptive Normalization"
date:   2019-05-20 09:10:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning generative style_transfer normalization conditional_img2img
---

<a href="https://arxiv.org/abs/1903.07291" target="_blank">https://arxiv.org/abs/1903.07291</a>


* multi-scale D, same loss as the pix2pixHD (replaced lsgan with hinge loss)
* 2-conv layers, spatially (for each channel, for each spatial location y, x) different denormalization
* tested `WGAN`, `cGAN` on \\(D\\) but not that effective, compared to the GPU memory consumption

### Abstract

* __spatially-adaptive normalization__ (`SPADE`): normalization layer for synthesizing photorealistic images given an _input semantic layout_ (=segmap)
* previous methods feed segmap to conv layer
    * > We show that this is suboptimal as the normalization layers tend to "wash away" semantic information" (no proofs though)
* propose using the input layout(segmap) for modulating the activations in normalization lyers through a spatially-adaptive (how?), learned transformations (conv)

### Intro
* References on earlier conditional image synthesis
* `Semantic Image Synthesis` : CRN, pix2pix, pix2pixHD
    * We show that the CNN (in this problem) is atmost sub-optimal, b/c their __norm layers tend to wash away' info in input semantic masks.__
    * to address this, we propose `SPADE`, a conditional norm layer that modulates the activations using input semantic layout througha spatially adaptive, learned transformation and can effecitvely propagate the semantic information throughout the network.
    * We show that with `SPADE` layer, a __compact network__ (how compact?) can synthesize significantly better result socmpared to several SOTA methods.

__Unconditional normalization layer__
* BN, IN, LN, WN
* do not depend on external data in constract to the conditional normalization

__Conditional normalization layer__
* CBN [10] (A leared representation), (AdaIN) [17]
* Both were first used in the style transfer task and later adopted in various vision tasks [9, 18, 29, 31, 34, 
35]
* normalization - applying affine transform using externel data
* for style trasfer tasks [10, 17], the affine params are used to control the golbal style for he output, and hence are __uniform across spatial coordinates.__
* `SPADE` applies a spatially varying affine transformation, making it suitable for image synthesis from __spatially-varying__ (for each feature map location + channel) semantic mask 

### Semantic Image Synthesis


Let \\(m\in L^\{H\times W\}\\) be a semantic seg mask, where \\(L\\) is the set of L integer \\(\\{0,1,...,L-1\\}\\) each denoting the ith label of segmentation. We wish to learn a function
\\[f : L\mapsto I\\] where \\(I\\) is a photographic image.

#### Spatially-adaptive denormlaization

SPatially Adaptive (De)normalization (SPADE) performs
1. channel-wise normalization (like BN)
2. Denormalization wrt channel + spatial location
\\[\gamma\_\{c,y,x\}^i(m)\frac\{h\_\{n,c,y,x\}^i-\mu\_c^i\}\{\sigma\_c^i\}+\beta\_\{c,y,x\}^i(m)\\]
where
* \\(h\_\{n,c,y,x\}^i\\) : feature map of ith layer, nth batch, pos at (y, x)
* \\(\mu\_c^i, \sigma\_c^i\\) : normalzation stats, calculated similarly to the conventional BN
* \\(\gamma\_\{c,y,x\}^i, \beta\_\{c,y,x\}^i\\) : _learned denormalization params__. Learned through feature-map sized downsampled segmap -> 2 conv layers
* __is a generalization of many conditional normalization layers__
    1. replace seg img with cls label, and make SPADE spatially invariable : conventioan Conditioanl BN
    2. replace seg img with other RGB img (style img), making the modulation params spatially invariant and setting \\(N=1\\) - `AdaIN`!


#### Spade generator
- no need to feed segmap to the 1st layer of G, since the learned params have encoded enough information about the label layout
- unlike pix2pix(HD), removed encoder part
- now, similarly to cls-conditional Gs (cGAN, SA-GAN), we can inject randn noise as input, "enabling a simple and natural way for multimodal synthesis [18, 50]

<img src="{{site.url}}/images/deeplearning/gan/spade_g.png" width="1200">

* above G employs several ResNet blocks with upsampling layers.
* Trained G with the same __multi-scale D__, loss functions used in pix2pixHD except that we replace the lsgan loss term into hinge loss term [25, 30, 45]
* tested several SOTA \\(D\\) in unconditional GAN, but not that efficient (compared to GPU memory required) - includes `WGAN`, `cGAN`
* adding SPADE to \\(D\\) was not that good
* removing any loss in pix2pixHD resutled in severe degradation


#### Why SPADE works better?

it can better presetve semantic information against common normalization layers


Consider a sumple module (segmap) -> conv -> bn.
1. assume a seg mask with a single label
2. wrt any (uniform) seg label, the output will be also uniform
3. applying insance normalization gives uniform output (0-centered) - loss of semantic information


<img src="{{site.url}}/images/deeplearning/gan/spade_explanation.png" width="600">  
In fig3, we empirically show this is precisely the case for pix2pixHD
> B/c a seg mask consists of a few uniform regions in general, the issue of information loss emerges when applying normalization

In constrast, the segmap is feed to SPADE module __without normalization__. Only activations from the prev layer are normalized. Hence the SPADE \\(G\\) can better preserve semantic information. It enjoys the benefit of normlaization without losing the semanic input information.

#### Multi-modal synthesis
- trained encoder with KL-divergence [22]

### Experiments
- Spetral norm, ADAM, sync BN
- COCO-Stuff / ADE20K(-outdoor) / Cityscapes, Flicker Landscapes

* Evaluation with pretrained Segnet inference - [7, 40]  
based on the intuition that if the output imgs are realistic then a well-trained semantic segmentation mdoel should be able to predict the gt label
* MIOU. DeeplabV2 for COCO-stucc, Upernet for Ade20k, DRN-D-105 for Cityscapes
* FID on cityscapes??!?
* Human perception study (AMT)
* SPADE ablation study
* We also find that concatenating seg masks at all intermediate layers, an intuitive alternative to SPADE to provide semantic signal, does not acheive the same performance as SPADE.


<br/><br/>
Next
`CBN`
* [9] : Modulating early visual processing by language
* [18] : multimodal unsupervised img2img translation, ICCV 2017
* [29] : Which training methods for gans do acually converge
* [31] : cGANs with projection discriminator, ICLR 2018
* [34] : Learning visual reasoning withou string priors
* [35] : Semi-parametric image synthesis, CVPR 2018

RandN input is a simple and natural way for multimodal synthesis
* [18] : Multimodal unsupervised image-to-image translation,k ECCV 2018, NVIDIA
* [50] : Toward multimodal img2img translation