---
layout: post
title:  "Bayesian Inference (+ beta distribution)"
date:   2020-12-04 08:00:05 +0800
categories: probability
use_math: true
tags: math probability bayesian
---


> <a href="https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/" target="_blank">part-ii-inference-limit-theorems</a>

Bayesian Inference
- __central idea__: always use Bayesian rule to find the posterior distribution \\(p\_{\theta\|X}\\)
  - uses prior \\(p\_\theta\\) and observation of the RV \\(P_{X\|\theta}\\).
- summarizing posterior : MAP (report maximum prob) / mean (report mean)

************************************

model building vs inference
1. model building
- which model to describe observation?
- often includes inferring (model params)
2. inferring unobserved vars
- infer hidden params (~model building) / hidden input (given output, model and params)

************************************

hypo testing vs estimation
1. hypo testing
- unknown takes one of few possible values, select the most probable one
2. estimation
- numerical, continuous.
- finding estimate that is close to the true unknown value

************************************

### Bayesian Inference framework
- \\(\theta\\) : unknown RV
  - often constant in other frameworks
- \\(f\_\theta\\) = `prior` (our belief before seeing data)
- __prior__ \\(f\_\Theta\\) __+ model of the observation__ \\(p\_{X\|\Theta}\\) __-----Bayes rule-----> posterior__ \\(p\_{\Theta\|X}\\)
   - `point estimation` : provide single value \\(\theta\\) which best explains the posterior
     - `MAP` : maximum a posteriori
        - provides __smallest probability error__ \\(P[\hat{\theta}=\Theta\|X=x]\\) for each observation \\(x\\)
        - which leads to the smallest `overall probability of error` \\[\sum P[\hat{\Theta}\neq\Theta\|X=x)p_X(x) = \sum P[\hat{\Theta}\neq\Theta\|\Theta=\theta)p\_\Theta(\theta)]\\] (it can be calculated in two ways) 
     - `Least Mean Squares` = provides smallest mean squared error \\(E[X\|\theta] = \mathop{argmin}\_\{c\}E[(X-c)^2]\\)
- `estimate` \\(\hat{\theta}=g(x)\\): scalar 
- `estimator` \\(\hat{\Theta}=g(X)\\): a RV. A rule we use to process the data


### Unknown Bias of n Coins toss and Beta distribution

Let there be 
* a __randomly biased__ coin, where the head probability is a RV \\(\Theta \in [0, 1]\\)
* prior \\(f\_\Theta(\cdot)\\)
* we perform \\(n\\) __(scalar)__ coin flip, and let \\(K\\) be the number of heads (RV)

{:.acounter}
1. Firstly assume that __prior__ \\(f\_\theta\\) __is uniform__ in \\([0, 1]\\). Then the posterior calculation will be
   \\[f\_{\theta\|K}(\theta\|k) = \frac{1 \cdot \binom{n}{k} \theta^k(1-\theta)^{n-k}}{p\_K(k) = \int f\_\Theta(\theta)f(k\|\theta)d\theta}\\]
   We are insterested in terms which are dependent on \\(\theta\\). Then
   \\[f\_{\theta\|K}(\theta\|k) = \alpha \cdot \theta^k(1-\theta)^{n-k}\\]
   where \\(\alpha\\) is a normalization constant. It is the form of `beta distibution`, \\(\beta(k+1, n-k+1)\\).
2. Now assume that the __prior is also a beta distribution__, \\(\beta(\alpha-1, \beta-1)\\). Then
   \\[f\_{\theta\|K}(\theta\|k) = c \cdot \theta^\alpha(1-\theta)\beta\theta^k(1-\theta)^{n-k}\\]
   we see that the posterior takes the form of beta distribution. Under this setting it is possible to __recursively update the posterior based on the observation__
{:.acounter}


Now we want to derive point estimate. Firstly assume uniform prior.

1. MAP estimate : find value of \\(\theta\\) which maximizes posterior
    - apply log on the posterior, and use optimality condition  
      \\[\frac{\partial}{\partial\theta} \left[  k\log\theta + (n-k)\log\theta \right] = \frac{k}{\theta}-\frac{n-k}{1-\theta}=0\\]
    - \\(\hat{\theta}\_\{\text{MAP}}=k/n\\)  
    -\\(k/n\\) : reasonable! (# of observed head / total tosses)  
    - `estimator` : \\(K/n\\). a RV (a function \\(K\\))  
2. Least Mean Squares \\(E[\Theta \| K=k] = \int_0^1 \theta f\_{\theta\|K}(\theta \| k)d\theta \\)
   - Integral would be \\[\int_0^1 \theta^{k+1} \theta^{k+1} (1-\theta)^{n-k} d\theta\\]
   - Note that, \\[\int\_0^1\theta^{\alpha}(1-\theta)^\beta d\theta = \frac{\alpha!\beta!}{(\alpha+\beta+1)!}\\]
   - the integral now becomes
   - \\[\frac{1}{\frac{k!(n-k)!}{(n+1)!}}\cdot \frac{(k+1)!(n-k)!}{(n+2)!}\\]
   - with cancelations, \\[E[\Theta\|K=k] = \frac{k+1}{n+2}\\]
3. If \\(n>>0\\), both would be similar

### The Beta formula
\\[\int\_0^1\theta^k(1-\theta)^{n-k}d\theta = \frac{k!(n-k)!}{(n+1)!}\\]

We can derive above formula with integral by parts, but there is an alternative, probabilistic arguments

{:.acounter}
1. We will use slightly different forumlation \\[\int\_0^1\alpha^k(1-\theta)^{n-k}d\beta = \frac{\alpha!(beta)!}{(\alpha+\beta+1)!}\\]
2. Let \\(X\_1,...,X\_\alpha,Z,Y\_1,...,Y\_\beta\\) be independent, uniform in [0, 1].
3. \\(P(X\_1<X\_2<...<X\_\alpha < Z < Y\_1 < Y\_2 <...<Y\_\beta) = P(A) = \frac{1}{(\alpha+\beta+1)}\\) - among \\((\alpha+\beta+1)!\\) permutation (ordering), we have only 1 such case
4. or, use the total probability theorem \\(P(A) = \int P(A\|X=\theta) f\_z(\theta)d\theta\\)  
    -1. \\(P(A\|X=\theta) = P(X\_1,...,X\_\alpha < \theta \\ \&\\  Y\_1,...,Y\_\beta > \theta \\ \&\\  \text{Xs, Ys are sorted)}\\)  
    -2. We see that, \\[P(X\_1,...,X\_\alpha<\theta ) = \theta^\alpha, P(Y\_1,...,Y\_\beta < \theta) = (1-\theta)^\beta\\]  
    -3. Regardless of the conditions, probability that every \\(X\_i\\) is sorted is \\(\alpha!\\), b/c every \\(X\_i\\) is equally probable. Vice versa for \\(Y\_i\\)s.  
    -4. Therefore, \\[P(A) = \int P(A\|X=\theta) f\_z(\theta)d\theta = \int \theta^\alpha(1-\theta)^\beta\frac{1}{\alpha!}\frac{1}{\beta}d\theta \\]  
5. Now we have two different represntation for \\(P(A)\\), and following equality gives the answer
{:.acounter}