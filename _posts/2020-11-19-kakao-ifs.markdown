---
layout: post
title:  "kakao ifs"
date:   2020-11-18 05:04:00 +0900
categories: coding
use_math: true
tags: coding python
---


<a href="https://if.kakao.com/" target="_blank">https://if.kakao.com/</a>


### 개인화 "콘텐츠 푸시" 고도화 후기 (2020)
- <a href="https://if.kakao.com/session/93" target="_blank">https://if.kakao.com/session/93</a>
- 콘텐츠 선정 / 발송량 선정 / 선호도 예측 / 자동화


#### 문제정의, 설계
- `컨텐츠 푸시`의 이유 : 유저 활성화
  1. 활성화 유형 1 : 비활성 유저 활성화 __(발표 주제)__
    - 유저에 대한 접근체널이 거의 푸시밖에 없다고 함
  2. 활성화 유형 2 : 기존 유저 활성화
    - 푸시 이외에 다른 체널이 있다고 함
- 활성화 상태의 정의 := 주1회 이상 접속
- `WAU` : weekly active user, 주간활성유저

***********************

비활성 유저
- 푸시 클릭율이 떨어짐
- `누구나 좋아할 만한 컨텐츠` (?) 추천이 일단 좋긴 함
  - 빅이슈 (정상회담), 경사
  - __흔하지 않다고 함__ (희소하므로 좋은 것)
  - 여기에 의존적이지 않은 푸시가 필요

> 목표 : 누구나 좋아할 콘텐츠를 보냈을 떄와 같은 수준의 WAU달성


#### 문제해결

전략, 목표
- 유저별 푸시는 주1회로 제한
- WAU수치 자체를 직접 최적화하긴 어려움
  - __대리 지표__ 사용 (proxy metric)
  - `클릭율 개선치` : 현재 컨텐츠 푸시 클릭율 / 대상 유저들의 평소 푸시 클릭율
    - 클릭율이 낮은 유저들이  클릭했을 시 높아짐
    - 목표 수치 : 2.4 (평소 클릭율 대비 2.4배 증가)
- 전략 : 개인화

__자동화 푸시 시스템__ : 컨텐츠 선정 \\(\rightarrow\\) 개인화 선호도 예측 \\(\rightarrow\\) 발송량 설정/발송 \\(\rightarrow\\) 결과 분석 \\(\rightarrow\\) 처음으로
1. 컨텐츠 선정
  - 컨텐츠 생성 30분후 푸시 발송여부 판단
  - 제목 + 본문 기반 모델링
  - `gradient boosting` ?
  - 컨텐츠 별로 발송량을 설정함? (200~300?) 유저가 기준이 되어야 하는 것 아닌가? 
  - 제목 자동 설정 : 템플릿 기반, A/B테스팅을 통해 결정
2. 개인화 선호도 예측
  -  각 컨텐츠별로 발송량 내에서, 선호도가 높은 유저에게 보냄
  -  content based (word embedding)
  -  `CTR` (click through rate) prediction
     -  과거 클릭 모델링. collab filtering 인듯
  - lookalike
    - '해당 컨텐츠에 대한 선호가 확실하게 판단되는 유저군과 유사한 유저군을 찾는 방식'
    - 유저 기반 nearest neighbor?
3. 컨텐츠의 발송량 설정 (__재발송이 성능 향상에 기여가 컸다고 함__)
  - __처음에는 소량,__ 유저반응 확인 (클릭율 개선치), 좋으면 발송량 늘림 (`feedback loop`)
    - 컨텐츠가 시즌성(?)이 강함 : `temporal dynamic`이 큰 듯. 따라서 처음에 일부 유저만 대상으로 소량만 보냄
    - 목표치보다 낮아도, 신뢰구간 안에 목표치가 들어오면 다시 보내본다고 함
    - 신뢰구간 : 정규분포 cdf 사용 (central limit theorem)
4. 기타 이슈
  - 비활성 유저들의 모델링이 어려움
  - 푸시 피드백 속도가 느림
  - 컨텐츠 수명이 짧음 (뉴스 라이프사이클 = 3~4시간)


### Buffalo (2019)
- `추천 시스템 : 사용자에게 콘텐츠 혹은 정보를 소비하는 경험을 제공하는 기술`
- matrix factorization
- 비교분석 : `Apache Spark` (Scala), `Quora QMF` (C++), `Implicit` (C++/Python) 등과의 비교
  - CPU, GPU모드에서 수 배 이상 빠르다고 함
  - `KakaoReco730M` : 10기가단위의 대량 데이터, batch based learning (Implicit이외에 지원 X)을 해야 한다고 함
    - 메모리 크기를 너무 늘리면 I/O 오버헤드 때문에 오히려 느려진다고 함
- priority: scalability가 최우선 (만족X시 적용자체가 불가능), 성능이 그 다음이었다고 함
  - __원하는 크기__ 의 데이터를 __적절한 시간__ 에 __효과적인 자원__ 으로 분석할 수 있는 프로젝트가 없어서 개발했다고 함
- 유사 콘텐츠 추천 / 개인화 추천에 대한 병렬 처리 기능 제공
  - 멜론 사용자 수천만명에게 10개만 추천하려고 해도 너무 오래 걸림 : inference service를 위한 병렬처리 제공
    - Approximate-KNN? 
- __최적화된 Python/C++ 코드베이스와 병렬처리를 효과적으로 구현__
  - python/c++ : gil처리는 Cython의 nogil context로?
  - openMP의 dynamic scheduling이외에 개선점?
- 청크 구조의 데이터 배치 스트림을 통한 메모리 관리 (h5py의 청크?)
- 기본적으로 sparse matrix대응이 되어있다고 함. sparse mat 행렬곱을 못 본거 같은데..못찾았거나 라이브러리에서 해 주는 건가?
- `matrix market` : sparse mat 표현인 듯
- validation시 temporal dynamic을 고려하는 것이 중요하다고 함


### 둥꿍둥꿍 느낌 아는 음악 바텐더 (2019)
- collab filtering > content based (보통)
  - 이지만 content based도 사용되는 부분이 있음

`content based + end to end`
  - VGG-like feature extractor (conv2d + max pool)
    - 음악을  자른 것을 퓨리에변환 (+로그 스케일링)한 `mel-spectrogram` \\(\rightarrow\\) high_lvl features(감성tag) predict / fc전 임베딩이 비슷한 곡 추천 
    - 크게 좋지는 않았다고 함
    - 음악을 구분하는 데는 효과적인 embedding들이 좋은 음악을 추천하는 데는 크게 유용하지 않았다고 함 (?)
      - 음식(커피?) 재료와 결과물이 정비례하지 않는 것 처럼..

`collaborative filtering`
- 일반적이고, 유저 + 아이템이 필요한 matrix factorization 대신 autoencoder 사용 (복원에 유용한 feature를 사용)
- (list of songs + list of words(=tags)) \\(\rightarrow\\) (list of songs + list of words(=tags)) (주어진 곡가 비슷한 느낌의 playlist 생성?)
  - 선택한 곡 목록 (playlist?) 기반 autoencoder - feature vector추출
    - `denosing` autoencoder (`denosing task`: 일부를 가리고 (dropout), 복원하는 task)
    - + tag (multitask learning)
      - 태그를 입력에 넣을 수도 있고, 아닐 수도 있음
      - 태그는 DJ playlist로부터 
  - 인코더 가운데에서 임베딩을 추출한 듯
  - playlist의 음악이 많을 시, word2vec의 negative sampling으로 근사적인 loss를 구할 수도 있으나, Euc norm의 W^TW를 precompute해서 효율화했다고 함?
- 인코더의 임베딩 - cosine 유사도로 곡간 유사도, 태그 간 유사도, 곡-태그(?), 태그+태그(?) 등을 활용?


### 추천 시스템 맥락과 취향 사이 (2020)
- `픽코마` : 일본 웹툰
  - `팝업 추천`
- 팝업 노출 시점 : 열람을 멈추고 나가려고 할 때
  - 새로운 작품 발견에 열려있는 상태?
- 아직 잘 열리지지 않은 작품을 의도적으로 노출하기도 했다고 함
- 빈익빈 부익부 현상
  - 인기 많음 - 많은 로그/데이터 - 추천이 더 많이 됨
- 추천의 자동화
  - 기존 : 수동으로 팝업 추천 (스케일 아웃이 어려움)
  - 통계적 유저선호를 반영하기 어려움

******************
추천방법
- 개인화 추천
  - 유저마다 취향 반영
- 연관 추천 : 현재 아이템과 연관성이 높은 아이템 추천
- 개발할수록 개인화 추천 \\(\rightarrow\\) 연관 추천으로 방향이 바뀜 (로맨스를 보는데 호러 추천은 어색함)
- `팝업이 노출되는 타이밍엔 취향/맥락 중 무엇이 중요할까?`
  - 둘 다 A/B 테스트로 비교를 해 봄
  - 최종적으로 Matrix factorization 활용
    - __유저와 아이템을 동일한 latent space에 매핑하는 것__ \\(\rightarrw\\) cos similarity 등을 이용한  
    - 1개의 모델을 가지고 개인화 / 연관 추천을 모두 구현할 수 있음
      - 개인화 추천 : 개인의 latent vector와 similarity가 높은 아이템 추천
      - 연관 추천 : 유저벡터는 안쓰고 아이템벡터로 추천

******************
취향 vs 맥락
1. 평가지표
  - 노출 (Impression) : 팝업 추천에 유저가 노출
  - 전환 (Conversion ): 노출된 추천을 클릭
  - 전환율 (CVR) : 전환수/노출수
2. 비교결과 : 개인화가 CVR+18
  - matric fac으로 도출된 유사도 (cos 유사도) 가 CVR에 어떻게 영향을 미쳤나
  1. x축 : 열람(중?) 작품의 latent vector와 유저의 latent vector간의 cos sim, y축은 각 추천에 따른 전환율
   - 연관 추천 : x, y축간에 강한 양의 상관관계
   - 개인화 추천 : 오히려 약한 음의 상관관계 (__직전 작품__ 에 크게 영향을 받지 않음)
  2. 팝업 작품과 열람 작품의 유사도 (x축 : 열람작품 - 팝업작품 간의 유사도, y축 : 각 추천에 따른 전환율)
   - 연관 추천 : 양의 상관관계는 있으나 높지 않음 (보던 작품이 마음에 들지 않으면, 개인화를 고려하지 않고 유사한 작품만 추천해도 전환율이 높지 않음?)
   - 개인화 추천 : 양의 상관관계도 있고, bias자체가 높음
  3. 요약 : 맥락은 전환율에 영향을 미치지만, 보던 작품이 마음에 들어야 잘 동작. 개인화는 골고루 잘 동작 `(취향 > 맥락)`
3. but 뷰어 엔드 추천에 개인화 추천 적용 \\(\rightarrow\\) 전환율 감소
  - 뷰어 엔드의 경우 작품을 끝까지 읽어야 추천에 노출 - 작품이 나름 마음에 들어야 끝까지 앍음 (연관추천이 좀 더 효올적)
4. 인사이트
  - 연관 추천을 20%이상 열람했을 시 추천함. but
  - 시간대에 따라 사용자 패턴이 달라지고, 효율적인 추천 또한 달라짐
    - 차후 열람 : `missing pop` (노출됐을 때는 안봤지만 나중에는 봄) - 추천 시간대 (컨텍스트?) 가 좋지 않은 것을 추천
    - 7시 (컨택스트가 좋지 않은 시간대) 에 팝업을 막아 다른 시간대에 추천 - 유의미한 CVR증가를 보임
5. 팝업 추천의 성과/가치
  1. 자동화
  2. 성능상승 (노출 -21.3퍼지만 전환율은 2배이상 상승)
  3. 스케일 업 후 112%의 전환 증가
  4. 홈추천 / 유사추천 등에 비해 높은 정량적 수치


### Python Application Server for Recommender System
- <a href="https://www.youtube.com/watch?v=6oOQJtLa14U" target="_blank">https://www.youtube.com/watch?v=6oOQJtLa14U</a>
- <a href="https://www2.slideshare.net/kimkwangseop/pycon-korea-2018-python-application-server-for-recommender-system" target="_blank">https://www2.slideshare.net/kimkwangseop/pycon-korea-2018-python-application-server-for-recommender-system</a>


Intro
- 추천 시스템 = 데이터를 소비하는 새로운 경험?
- 추천 : 아이템 추천 / 개인화 추천 (시스템적으로 크게 다르지 않음)
  - 아이템 추천 시나리오 : 쿼리 \\(\rightarrow\\) 유사도 정렬 \\(\rightarrow\\) 메타데이터/선호도 조회 \\(\rightarrow\\) 필터링 (개인선호도 / 재고 등의 temporal) \\(\rightarrow\\) 추천
    - 필요한 것 : 개인의 활동 정보, 관계/유사도, 상품정보


APP 서버를 파이썬으로 (schema(rule) 기반 추천 제공)
- 룰/명세 기반으로 동작하는 파이썬 서버
- 요구사항 : 3가지 영역
  1. DB (유사도/메타데이터/이력 데이터) : 수평확장, 대량의 R/W (nosql)
  2. `OLTP` : Online Transaction Processing. 관계형 분석 기능 (where절, join 등?)
  3. 실시간 분석 (includes personalized recommendation), 복잡한 비즈니스 로직?
- 실시간 분석 / OLTP 최적화가 목표
  - 관계형 DB로 처리하기 미묘한 작업들 _(ex - 제외가 아니고 우선순위를 뒤로 밀어내기)_, 대용량/실시간 학습 대응
  - 다른 서비스가 사용하기 쉽게 API로 만들어야 한다고 함

> 개인화 추천을 미리 계산하는 것은 비효율적, 실시간 분석이 적합 (사용자=수천만, 아이템=수백만)

#### Arch
<img src="{{site.url}}/images/recomm/brunch_recomm_arch.jpg" width="700">

- 각 container당 4개정도의 python procs
- 새로운 서비스 = 스키마 추가 (생산성)
  

#### 개발 후기
Python2 Tornado vs python3 Sanic
  - Tornado legacy가 있었지만 3 Sanic 선택
  - 코드 가독성, 양이 줄음
  - Tornado는 low lvl로 내려가기 쉽지만 코드가 비교적 복잡하다고 함
  - Tornado : raise로 리턴? 
  - Sanic의 uvloop (이벤트 루프)가 좋은 이벤트 루프라고 함
- `asyncio.Future`
  - 단일 함수 내에서 순차적 await - 다른 함수와의 비동기 연산은 되지만 현재 함수에서의 병렬화는 안됨. 어차피 IO연산들 (uninterruptible sleep) 이라면 future로 단일 함수 내부에서도 병렬로 실행하는 것이 좋음

********************************

DB profiling
  - 대부분의 시간이 network IO였다고 함
  - DB 프로파일링 결과
    1. MongoDB의 aggregating : 여러 연산의 sequential 실행  
        - filter연산은 App 서버에서 하는 것 보단 DB단에서 하는 것이 좋음
        - DB에서 group by는 엄청 느렸는데 app서버에서가 훨씬 빨랐다고 함
        - DB를 너무 믿지 말자

********************************

caching
  - db조회 비용이 비싸서 캐시 hit을 최대화
  - 쿼리도 쪼개서 가능한 cache hit을 높임
  - redis 1 : python instances N
    - 프로세스간의 중복 요청도 cache로 처리 가능
  - 슈퍼노드 이슈 (?)
    - DB는 shard를 만들어 데이터를 쪼개서 저장
    - `적절한 키로 샤드를 구성하더라도, 공통되어 사용되는 리소스가 존재하는 경우 특정 샤드에 부하가 집중된다고 함`
      - 핫한 아이템이 있는 경우,
    - 인스턴스 내에서 공용 캐쉬 (python instances N개 내에서 공유)를 써서 어느정도 대응이 되었다고 (caching) 함 
- 커스텀 필터 구현 (DB?)
- 룰에 파이썬 코드를 심고 런타임 컴파일(?) 을 사용 (처음부터 배포하는 것 보단) 
  - `@filter : lambda x : ... ` 같은걸 받으면 eval해서 적용한 건가? ㄷㄷ
- 버킷 테스트?

********************************

설정 재배포
1. 모든 proc을 동시에 재시작 (안좋음)
2. 1개씩 PID 찝어서 재시작
3. WSGI 를 쓰는 장점?
    - `GUNICON` 에 kill sigterm - 알아서 graceful shutdown을 시켜준다고 함
      - 각 인스턴스마다 떠 있는 gunicon에 signal만 보내주면 된다고 함
      - 문서의 `Signal Handling` 부분에 Graceful shotdown

********************************

CPU연산
- Numpy, Scipy : openblas, sims 등을 사용한 컴파일까지 하면 C코드랑 큰 차이가 없다고 함


이벤트루프 상에서 스위칭 시 memcpy가 발생하는 경우?
- 큐를 사용했을 시
  - 프로듀서 : 컨슈머가 다른 상황에서 발생한다고 함
  - 프로세스가 달랐으면 IPC를 통해서 데이터 카피가 일어난다고 함

`failover` (?) 첨들어봄
- DB의 failover는 몽고디비의 failover를 따라감
- Instance 내의 failover / nginx