---
layout: post
title:  "Moment Generating Functions"
date:   2020-12-02 08:00:05 +0800
categories: probability
use_math: true
tags: math probability 
---
{:.acounter}

- <a href="https://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf" target="_blank">chap 4</a>
  - MGF provides an alternative representation of its probability law (PMF or PDF)
    - often convenient for certain types of math manipulations

`moment generating function` of a random variable \\(X\\) is a function \\(M\_X(s)\\) of a free parameter \\(s\\), defined by
\\[M_X(s) = E[e^{sX}]\\]

when \\(X\\) is __discrete__
\\[M_X(s) = \sum\_xe^{sx}p\_X(x)\\]
while in the __continuous case__
\\[m\_X(s) = \int\_{-\infty}^\infty e^{sx}f\_X(x)dx\\]

example. Let
\\[ p\_X(x) = 
\begin\{cases\}
1/2, & \text\{  \} x=2 \cr
1/6, & \text\{ if \} x=3 \cr 
1/3, & \text\{ if \} x=5
\end\{cases\}
\\]
Then the corresponding MGF is
\\[M(s) = \frac\{1\}\{2\}e^{2s}+\frac\{1\}\{6\}e^{3s}+\frac\{1\}\{3\}e^{5s}\\]

- MGF of Geometric \\[M\_X(s) = e^{\lambda(e^s-1)}\\]
- MGF of Exponential \\[\frac{\lambda}{s-\lambda}\\]
- proof : refer to the pdf in link
  
### Transform of a Linear Function of a RV

Let \\(Y=aX+b\\), where \\(X\\) is a RV. Then
\\[M\_Y(s) = E[e^{s(aX+b)}] = e^{sb}E[e^{saX}]=e^{sb}M\_X(sa)\\]

ex) If \\(X\\) is exponential with \\(\lambda=1\\), and if \\(Y=2X+3\\), then \\(M\_Y(s)=e^{3s}\frac{1}{1-2s}\\) 

Application
- MGF of standard normal \\(Y\sim N(0, 1), f\_Y(y) = \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\\) is \\(e^{s^2/2}\\) (proof : link pdf, example 4.5)
  - then any normal with \\(X\sim N(\mu, \sigma^2)\\) is \\[M\_X(s) = e^{s\mu}M\_Y(s\sigma) = e^{\frac{\sigma^2s^2}{s}+\mu s}\\]

### Generating Moments from MGF
\\[\frac{d^n}{ds^n}M(s)\|\_{s=0}  = \int\_{-\infty}^{\infty}xf\_X(x)dx = E[X^n]\\]
- the derivation (in pdf) involves an `interchange of differentiation and integration` (__TODO__ : make a new post on it!)


### Inversion property
> The MGF \\(M\_X(s)\\) completely determines the PDF/PMF of \\(X\\) : there is 1:1 bijection

(in fact, the bijection holds true when we only know \\(M(s)\\) for certain positive length inverval) - 어디서 관련 내용 봤는데...


### MGF of a mixture of distributions
example) A bank has three tellers, two fast, one slow. 
- A time required to handle a customer is exponentiall distributed with \\(\lambda=6\\) for fast teller, and \\(\lambda=4\\) for slow teller
- A person enters and choose teller at random. let \\(X\\) be the time to handle one arbitrary customer.
- We have, \\[f\_X(x) = \frac{2}{3}6e^{-6x}+\frac{1}{3}4e^{-4x},\quad\quad x\geq 0\\]
  - Note that this case is somewhat different from the summation of RVs
- Then the MGF becomes
  \\[
  \begin\{align\*\}  
    M(s) &= \int\_0^\inf e^{sx}(\frac{2}{3}6e^{-6x}+\frac{1}{3}4e^{-4x})dx \cr
         &= \frac{2}{3}\int\_0^\infty e^{sx}6e^{-6x}dx + \frac{1}{3}\_0^\infty e^{sx}4e^{-4x}dx \cr
         &= \frac{2}{3}f\_{X\_1}(s) + \frac{1}{3}M\_{X\_2}(s)
  \end\{align\*\}    
  \\] where \\(X_1\sim \exp(6), X\_2\sim \exp(4)\\).

Therefore we see that, the pdf of a mixture of distribution 
\\[f\_Y(y) = p\_1f\_{X\_1}(y)+...+p\_nf\_{X\_n}(y)\\]
the MGF becomes
\\[M\_Y(s) = p\_1M\_{X\_1}(s)+...+p\_nM\_{X\_n}(s)\\]



### Sum of Independent RVs
when independent, sum of RV - becomes multiplication of MGFs

example)
1. Bernoulli - Binomial
2. The sum of Independent Poisson is Poisson (the multiplication of MGF again as a form of Poisson)
3. The sum of Independent Normal is Normal (the multiplication of MGF again as a form of Normal)

### MGF of joint(multivariate) probability


<img src="{{site.url}}/images/math/prob/joint_mgf.jpg" width="800">  

왜 이렇지? 라플라스 변환이랑 무슨 관계가 있나?..

The `inversion property` again holds in multivariate case.

### Sum of Random Number of Independent RVs
Let \\(Y=X\_1+...+X\_N\\), where \\(N, X\_i\\)s are independent, and \\(X\_i\\)s are identically distributed.  
We want to calculate \\(M\_Y(s) = E[e^{sY}]\\) in terms of the MGFs of \\(X\_i, N\\).  
Our strategy would be \\[E[e^{sY}] = E[E[e^{sY}\|N]]\\]

1. \\(E[e^{sY} \| N=n] = E[e^{s(X\_1+...+X\_N)}\|N=n] = E[e^{s(X\_1+...+X\_n)}] = (M\_X(s))^n\\)
2. therefore we have, \\[E[e^{sY}\|N] = (M\_X(s))^N\\]
3. Using the iterated expectation,
    \\[E[e^{sY}] = E[E[e^{sY}\|N]] = E[(M\_X(s))^N]\\]
4. this is somewhat similar to the MGF of \\(N\\), which is \\(E[(e^s)^N]\\).  
   We can obtain the MGF of \\(Y\\) by changeing \\(e^s\\) from the MGF of \\(N\\) to \\(M\_X(s)\\).

Examples
1. `Sum of a geometric Number of i.i.d Exponential RV is Exponential`
  1. Jane visits a number of bookstores to find a book
  2. For each trial
     1. she finds the book with probability \\(p\\)
     2. or she cannot find the book, and stays there with exponentially distributed time \\(\lambda\\)
     4. Let \\(X\_i\sim \exp(\lambda)\\) be the waiting time on each bookstores, and are indenpent on other RVs
     5. Let \\(N \sim \text{geo}(p)\\) be the number of bookstores she visited
     3. let \\(Y:=X\_1+...X+\_N\\) be the total time she spent in bookstore. Then it is the __Sum of a Geometric Number of Independent Exponential__
     6. We see that, \\[M\_X(s)=\frac{\lambda}{\lambda-s},\quad\quad M\_N(s)=\frac{pe^s}{1-(1-p)e^s}\\]
     7. using above rule, we exchange every \\(e^s\\) on \\(M\_N\\) by \\(M\_X\\) and see that 
        \\[M\_Y(s) = \frac{pM\_X(s)}{ 1-(1-p)M\_X(s) } = \frac{p\lambda}{p\lambda-s}\\]  
        which is a MGF of \\(\exp (p\lambda)\\)
     8. Note that the fixed sum of i.i.d exponential is not an exponential
2. `Sum of a geometric number of i.i.d geometric is geometric`
    - can be proven in same way.