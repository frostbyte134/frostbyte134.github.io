---
layout: post
title:  "A Note on the Inception Score"
date:   2019-02-09 09:00:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning generative inception_score gan inception
---

<a href="https://arxiv.org/pdf/1801.01973.pdf" target="_blank">https://arxiv.org/pdf/1801.01973.pdf</a>


### Abstract
> ... demonstrate that it (Inception score) fails to provide useful guidance when comparing model.

* discuss both suboptimalities of the metric itself and issues with its application.


### Evaluating Black-Box Generative Models
Let
* \\(x\\): images, text, GPS, ...
* \\(p\_r(x)\\): unknown real data distribution
* \\(g\_g(x)\\): A distribution, encoded by our generative model \\(G\\)

The aim (of training generative models \\(G\\)) is to find \\(p\_g(x)\\) s.t. \\(p\_g(x)\approx p\_r(x)\\) according to __some metric__.

The situation can be divided into
1. \\(p\_g(x)\\) is explicitly available  
	\\(\rightarrow\\) For given dataset \\(X\\), choose \\(G\\) that maximizes the likelihood of \\(X\\) over \\(p\_g(x)\\).
	\\(\rightarrow\\) mostly not the case.  
	ex) - `GAN` implicitly maps random noise to samples through a parametrized NN.
2. Approximate \\(p\_g(x)\\)
	1. For images, <a href="http://www.iro.umontreal.ca/~lisa/publications/unlearning_for_better_mixing.pdf" target="_blank">`Parzen Window Estimate`</a> based approximation is possible (seems not that popular)
	2. Apply pre-trained NN, calc stats of its output / certain layers
	* EX) - <a href="Improved techniques for training gans. " target="_blank">`Inception Score`</a>(IS), <a href="https://arxiv.org/abs/1706.08500" target="_blank">`Frechet Inception Distance`</a> (FID)  
	* __(often motivated by demonstrating that)__ correlated with visual quality, prefers models that generates realistic images.

Further, there are several works concerned with the __evaluation of evaluation metrics__ themselves
1. Thesis et al
	* common eval metrics do not correlate with each other
	* generative models need to be directly evaluated for the application
2. <a href="https://arxiv.org/abs/1806.07755" target="_blank">Qiantong et al</a>
	* investigated several sample-based eval metrics
	* Maximum Mean Discrepancy and 1-Nearest neighbour two sample test satisfied most of the desirable properties of a metric

### Inception score
1. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" target="_blank">`Inception network`</a>: pretrained over imagenet / cifar, produces conditional prob over the classes
\\[p(y \| x) \in [0,1]^\{1000\}\\]
2. Inception score is defined by,
\\[\text\{IS\}(G) = \exp\left( E\_\{x\sim p\_g\}D\_\{KL\}(p(y \| x) \|\| p(y)) \right)\\]
where
\\[p(y)=\int\_xp(y\| x)p\_g(x)dx\\]
Note that \\(p(y)\\) is not calculable in most cases

The author of `IS` tried to codify __2 desirable quailties__ of a generative model into the metric
1. The image generated should contain clear objects  
\\(\approx\\) The image generated must be identifible by the pretrained network with high confidence  
\\(\rightarrow\\) \\(p(y\| x)\\) must be low entropy (far from uniform)
2. The generative algorithm must output high diversity  
\\(\rightarrow\\) \\(p(y)\\) must be high entropy (close to uniform)

Therefore, when \\(p(y\| x)\\) is low and \\(p(y)\\) (which is calculated with \\(p\_g(x)\\)) is high, then we have high `IS`.

Analysis
1. Expected KL-divergence between conditional distribution || marginal distribution = `mutual information`
\\[\text\{ln(IS(G))\}=E\_\{x\sim p\_g\}D\_\{KL\}(p(y \| x) \|\| p(y))\\] 
\\[=\sum\_x p\_g(x) \sum\_i p(y=i \| x) \ln \frac\{p(y=i \| x)\}\{p(y=i)\}\\]
\\[=\sum\_x \sum\_i p(y=i \| x) p(x,\\ y=i) \ln \frac\{p(y=i,\\ x)\}\{p(x)p(y=i)\}\\]
\\[=I(y;x)\\]
In words, \\(\text\{IS\}\approx\\) measure of discrepancy between the images generated by \\(G\\) (\\(p\_g(x)\\)) and marginal class distribution (\\(p(y)\\)).
2. The mutual information \\(I(y;x)\\) can be represneted by the entropies as
\\[I(y;x) = H(y) - H(y \| x)\\]
3. By the properties of entropies
\\[0\leq H(y) - H(y \| x) \leq H(y) \leq \ln(1000)\\]
	1. left 2 inequalities: entropy is nonnegative
	2. rightmost inequality: uniform distibution (over 1000 cls) is the unconstrained maximum entropy
4. Empirical calcumation
\\[\hat\{p\}(x)=\frac\{1\}\{N\}\sum\_\{i=1\}^N p(y\| x^\{(i)\})\\]	
\\[\rightarrow \text\{IS\}(G) \approx \exp\left( \frac\{1\}\{N\}\sum\_\{i=1\}^N p(y\| x^\{(i)\}) \right)\\]
original paper recommended N=5000, taking expectation of scores over 10 repititions.