---
layout: post
title:  "Generative Adversarial Nets"
date:   2018-12-07 09:00:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning gan domain 
---

<a href="https://arxiv.org/abs/1406.2661" target="_blank">https://arxiv.org/abs/1406.2661</a>


### Abstract
We simultaneously train two models:
* a generative model \\(G\\) __(counterfeit)__ that captures the data distribution, and
* a discriminative model \\(D\\) __(police)__ that estimates the prob mahat a sample came from the training data, rather than \\(G\\).

The `training procedure` for \\(G\\) is to maximize the prob of \\(D\\) making a mistake. This corresponds to a __minimax__ two player game.

In the space of arbitrary functions (non-parametric, which is quite unrealistic) \\(G, D\\), a __unique solution exists__, with 
* \\(G\\) recovering the training data dist., and 
* \\(D\\) equal to \\(1/2\\) almost everywhere (in the support of \\(G, D\\))

### Introduction

Deep generative models have had less of an impact, 
* due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood est. and related strategies, and
* due to difficulty of leveraging the benefits of piecewise linear units in the generative context.


### Related work
<img src="{{ site.url }}/images/deeplearning/gan/generative-2014.png" class="center" style="width:700px"/>   
In __undirected graphical models__ (MRF!) with latent variables such as RBM, or DBP (deep Boltzman machines), 
* interactions are represneted as the prod of unnormalized potentials.  
* The partition function and its gradient are intractable for all but the most trivial instances, amthough they can be estimated by MCMC methods.


### Adversarial nets

To learn the 
* \\(p\_g(x)\\), the generator's distribution over the data \\(x\\), 

we define
* \\(p\_z(z)\\), a prior on input noise variables (uniform RV in the paper), then by
* \\(G(z;\theta\_g)\\), represent a mapping to the data space from the random source (sth like compositing uniform [0,1] and inverse of PDF?),
* where \\(G\\) is a differentiable function represented by multilayer perceptron with __parameters__ \\(\theta\_g\\)

We also define a 2nd multilayer perceptron
* \\(D(x;\theta\_d)\\) that outputs a single scalar
* \\(D(x)\\) represents the __probability that__ \\(x\\) __came from the data,__ rather than \\(p\_g\\). 

We train \\(D\\) to maximize the prob of assigning the correct labels to both 
1. the sample from \\(G\\), and
2. the training data.  
We simultaneously train \\(G\\) to minimize \\(\log(1-D(G(z)))\\)

In other words, \\(D, G\\) play the following two-player minimax game with value function \\(V(G,D)\\):
\\[ \min\_G \max\_D V(D,G) = \mathbb\{E}\_\{x\sim p\_\{data\}(x)\}[\log D(x)] + 
\mathbb\{E}\_\{z\sim p\_z(z)\}[\log (1-D(G(z))]\tag\{1\}\\]

In practice, optimizing \\(D\\) to completion in the inner max loop is computationally prohibitive, and on finite datasets would result in __overfitting__ (why overfit?)  
\\(\rightarrow\\) instead, we alternate between \\(k\\) __steps of optimizing__ \\(D\\) and __one step of optimzing__ \\(G\\).  
This result in \\(D\\) being maintained near its optimal solution, so long as \\(G\\) changes slowly enough.

In practice, eq(1) may not provide sufficient grad for \\(G\\) to learn well. Early in learing, when \\(G\\) is poor, \\(D\\) can reject samples with high conf, b/c they are clearly different from the training data.  
In this case \\(\log(1-D(G(z)))\\) saturates (to 0?). Rather than training \\(G\\) to minimizing it, we can train \\(G\\) to maximize \\(\log D(G(z))\\). This objective func results in the same fixed point of the dynamics of \\(G, D\\) but provides much stronger grads early in learning


### Theoretical Results
The results of this section is done in __non-parametric__ setting, e.g. models with infinite capacity

### 1. Global optimality of \\(p\_g=p\_\{data\}\\)

`Proposition 1`: For \\(G\\) fixed, the __optimal discriminator__ \\(D\\) is
\\[D\_G^*=\frac\{p\_\{data\}(x)\}\{p\_\{data\}(x)+p\_g(x)\}\tag\{2\}\\]
(is this term sum to 1?)

__Proof__: The training criterion for the discriminator \\(D\\) given any generator \\(G\\), is to maximize the quantity \\(V(G,D)\\)
\\[ V(G,D)=\int\_x\{p\_\{data\}(x)\log(D(x))dx\} + \int\_z p\_\{z\}(z)\log(1-D(g(z)))dz\\]
by substituting \\(g(z)=x\\) and performing the integration over the \\(Supp(p\_\{data\})\bigcup Supp(p\_g)\\) (which is the domain of the discriminator WLOG),
\\[=\int\_x\{p\_\{data\}(x)\log(D(x)) + p\_g(x)\log(1-D(x))dx\} \\]

For any \\((a,b)\in \Re^2 \backslash \\{0,0\\}\\) (isn't the constraint that \\(a,b \in[0,1]\\) (since they are probs in above eq) or \\((a,b)\geq (0, 0)\\) (for following maximality to be hold) must be added?) achieves its maximum in [0,1] at \\(\frac\{a\}\{a+b\}\\), concluding proof.


`Note` that the training objective for \\(D\\) can be interpreted as maximizing the log-likelihood for estimating the conditional prob \\(P(Y=y\|X=x)\\), where \\(Y\\) indicates whether \\(x\\) comes from data (with y=1) or from G (with y=0).  
(To estimate such conditional prob (which should be binary valued, if optimal), we have to maximize the log-likelihood in \\(V(G,D)\\))


Now the minimax game in Eq (1) can be reformulated as \\(\min\_\{G\}C(G)\\) with,
\\[ C(G)=\max\_\{D\}V(G,D) \\]


Next:  


