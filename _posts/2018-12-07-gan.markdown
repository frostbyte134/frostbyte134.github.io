---
layout: post
title:  "Generative Adversarial Nets"
date:   2018-12-07 09:00:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning gan domain 
---

<a href="https://arxiv.org/abs/1406.2661" target="_blank">https://arxiv.org/abs/1406.2661</a>


### Abstract
We simultaneously train two models:
* a generative model \\(G\\) __(counterfeit)__ that captures the data distribution, and
* a discriminative model \\(D\\) __(police)__ that estimates the prob that a sample came from the training data, rather than \\(G\\).

The `training procedure` for \\(G\\) is to maximize the prob of \\(D\\) making a mistake. This corresponds to a __minimax__ two player game.

In the space of arbitrary functions (non-parametric, which is quite unrealistic) \\(G, D\\), a __unique solution exists__, with 
* \\(G\\) recovering the training data dist., and 
* \\(D\\) equal to \\(1/2\\) almost everywhere (in the support of \\(G, D\\))

### Introduction

Deep generative models have had less of an impact, 
* due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood est. and related strategies, and
* due to difficulty of leveraging the benefits of piecewise linear units in the generative context.


### Related work
<img src="{{ site.url }}/images/deeplearning/gan/generative-2014.png" class="center" style="width:700px"/>   
In __undirected graphical models__ (MRF!) with latent variables such as RBM, or DBP (deep Boltzman machines), 
* interactions are represneted as the prod of unnormalized potentials.  
* The partition function and its gradient are intractable for all but the most trivial instances, akthough they can be estimated by MCMC methods.


### Adversarial nets

To learn the 
* \\(p\_g(x)\\), the generator's distribution over the data \\(x\\), 

we define
* \\(p\_z(z)\\), a prior on input noise variables (uniform RV in the paper), then by
* \\(G(z;\theta\_g)\\), represent a mapping to the data space from the random source (sth like compositing uniform [0,1] and inverse of PDF?),
* where \\(G\\) is a differentiable function represented by multilayer perceptron with __parameters__ \\(\theta\_g\\)

We also define a 2nd multilayer perceptron
* \\(D(x;\theta\_d)\\) that outputs a single scalar
* \\(D(x)\\) represents the __probability that__ \\(x\\) __came from the data,__ rather than \\(p\_g\\). 

We train \\(D\\) to maximize the prob of assigning the correct labels to both 
1. the sample from \\(G\\), and
2. the training data.  
We simultaneously train \\(G\\) to minimize \\(\log(1-D(G(z)))\\)

In other words, \\(D, G\\) play the following two-player minimax game with value function \\(V(G,D)\\):
\\[ \min\_G \max\_D V(D,G) = \mathbb\{E}\_\{x\sim p\_\{data\}(x)\}[\log D(x)] + 
\mathbb\{E}\_\{z\sim p\_z(z)\}[\log (1-D(G(z))]\tag\{1\}\\]

In practice, optimizing \\(D\\) to completion in the inner max loop is computationally prohibitive, and on finite datasets would result in __overfitting__ (why overfit?)  
\\(\rightarrow\\) instead, we alternate between \\(k\\) __steps of optimizing__ \\(D\\) and __one step of optimzing__ \\(G\\).  
This result in \\(D\\) being maintained near its optimal solution, so long as \\(G\\) changes slowly enough.

In practice, eq(1) may not provide sufficient grad for \\(G\\) to learn well. Early in learing, when \\(G\\) is poor, \\(D\\) can reject samples with high conf, b/c they are clearly different from the training data.  
In this case \\(\log(1-D(G(z)))\\) saturates (to 0?). Rather than training \\(G\\) to minimizing it, we can train \\(G\\) to maximize \\(\log D(G(z))\\). This objective func results in the same fixed point of the dynamics of \\(G, D\\) but provides much stronger grads early in learning


### Theoretical Results
The results of this section is done in __non-parametric__ setting, e.g. models with infinite capacity

### 1. Global optimality of \\(p\_g=p\_\{data\}\\)

`Proposition 1`: For \\(G\\) fixed, the __optimal discriminator__ \\(D\\) is
\\[D\_G^*=\frac\{p\_\{data\}(x)\}\{p\_\{data\}(x)+p\_g(x)\}\tag\{2\}\\]
(\\(D(x)\\) corresponds to \\(P[Y=data \|\ X=x]\\), thus sum to 1 by \\(P[Y=data \|\ X=x]+P[Y=fake \|\ X=x]\\)

__Proof__: The training criterion for the discriminator \\(D\\) given any generator \\(G\\), is to maximize the quantity \\(V(G,D)\\)
\\[ V(G,D)=\int\_x\{p\_\{data\}(x)\log(D(x))dx\} + \int\_z p\_\{z\}(z)\log(1-D(g(z)))dz \\]
by substituting \\(g(z)=x\\) and performing the integration over the \\(Supp(p\_\{data\})\bigcup Supp(p\_g)\\) (which is the domain of the discriminator WLOG),
\\[=\int\_x\{p\_\{data\}(x)\log(D(x)) + p\_g(x)\log(1-D(x))dx\} \tag\{3\}\\]

For any \\((a,b)\in \Re^2 \backslash \\{0,0\\}\\)   
(isn't the constraint that \\(a,b \in[0,1]\\) (since they are probs in above eq) or \\((a,b)\geq (0, 0)\\) (for following maximality to be hold) must be added?)  
, \\(a\log y+b\log (1-y)\\) achieves its maximum in [0,1] at \\(\frac\{a\}\{a+b\}\\), concluding proof.

> `Note` that the training objective for \\(D\\) can be interpreted as maximizing the log-likelihood for estimating the conditional prob \\(P(Y=y\|X=x)\\), where \\(Y\\) indicates whether \\(x\\) comes from data (with y=1) or from G (with y=0).  

\\(\rightarrow\\) The training object independently maximizes(=estimates), for each \\(x\\), the conditional prob \\(P(Y=y\|X=x)\\)


Now the minimax game in Eq (1) can be reformulated as \\(\min\_\{G\}C(G)\\) with,
\\[ C(G)=\max\_\{D\}V(G,D) \\]

__Theorem 1__: The global minimum of the virtual training criterion \\(C(G)\\) is achieved iif \\(p\_g=p\_\{data\}\\). At that point, \\(C(G)\\) achieves the value \\(-\log 4\\).

Note that,
\\[p\_g(x)=P[G(z;\theta\_p)=x]:=\sum\_\{z\text\{  s.t.  \} G(z;\theta\_p)=x\}P[Z=z]\\]
, where \\(P[Z=z]\\) is the source of the randomness (U(0,1) maybe). Therefore given \\(p\_g\\), there is limited \\(G\\) which can achieve given \\(G\\).    
`proof`: For \\(p\_g=p\_\{data\}\\), \\(D^*\_\{G\}(x)=1/2\\). Hence by inspecting Eq.4, the function value is evaluated as \\(C(G')=-\log 4\\). To see that this is the best possible value of \\(C(G)\\), ...

(just note that KL-divergence is \\(p\log\frac\{p\}\{q\}\\), not \\(\frac\{\log p\}\{\log q\}\\)).


__4.2 Convergence of Algorithm 1__:  
For the convexity of \\(V(G,D)=U(p\_g,D)\\) in \\(p\_g\\). Remember (though skipped in this page) that in the Eq.(3),  
\\[ V(G,D)=\int\_x\{p\_\{data\}(x)\log(D(x)) + p\_g(x)\log(1-D(x))dx\} \tag\{3\}\\]


### Pros and Cons

__Disadvantages__: there is no explicit represnetation of \\(p\_g(x)\\), and that \\(D\\) must be synchronized well with \\(G\\) during training (in particular, \\(G\\) must not be trained too much without updating \\(D\\), in order to avoid `the Helvetica scenario` in which \\(G\\) collapses too many values of \\(z\\) to the same value of \\(x\\) to have __enough diversity__ to model \\(p\_\{data\}\\))

__Advantages__: no need of Markov chains. only baack prop/ on inference is needed during learning. 

Next:  


