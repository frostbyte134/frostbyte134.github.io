---
layout: post
title:  "Singular value decomposition"
date:   2022-11-28 13:13:00 +0900
categories: linear_algebra
use_math: true
tags: linear_algebra need_revise
---

2가지 방향으로 이해가능 (updated at 2021-11-28)
1. row들에 대한 best fit subspace를 찾는 문제로 - [아이패드 노트필기 문서 참고](https://drive.google.com/file/d/1-KVYlnwD2i5yrhkbWPi0w2n6VMSOtGzQ/view?usp=sharing). Matrix norm (Frobenious, l2)관점에서 SVD가 최적의 (norm을 가장 잘 보존하는) rank-K approximation을 (값이 낮는 singular value들을 쳐내서) 것 까지 스무스하게 보일 수 있음 (PCA까지 연결)
2. column관점에서, symmetric positive definite한 normal행렬 A^TA나 AA^T를 eigen decomposition한 것으로 이해하기. 구성적으로 이해하기 쉬워서 좋음. 아래 내용이 해당 관련
	- inverse mapping관점에서 SVD로 구한 pseudoinverse가 매팽해주는 곳이 nullspace component를 포함하지 않으므로, 벡터 놈 관점에서 가장 최적의 (작은) 곳으로 매핑해줌

------------------------------------------------

\\(U\Sigma V^T\\) joins \\(LU\\) from Gaussian elimination and <a href="{{site.url}}/linear_algebra/2018/05/15/orthonormal-basis.html" target="_blank">\\(QR\\) from orthogonalization.</a>  
For symmetric(Hermitian) matrix, \\(A=Q\Lambda Q^T\\) holds and everything is clear.  
However for __rectangular__ \\(A\\), eigenvalues are not even defined
\\[Ax\neq \lambda x\\]
\\[(m\times n)\times (n) = m \neq n.\\]
so that \\(A\neq Q\Lambda Q^T\\).  

In such case, we allow \\(Q=U,\\>Q^T=V^T\\) for some unitary \\(U\\) and \\(V\\), not necessarily in transpose relation. Then
\\[\textrm\{every matrix will split into \} A=U\Sigma V^T.\\] 

<h3 id="svd">Singular Value Decomposition</h3>
> Singular value decomposition (SVD): Any \\(m\times n\\) matrix \\(A\\) can be decomposed into \\[A=U\Sigma V^T = (m\times m\textrm\{ unitary \})(m\times n\textrm\{ diagonal\})(n\times n\textrm\{ unitary\}).\\]


1. \\(\Sigma:\\>m\times n\\). Has \\(r=\textrm\{rank\}A\\) nonzero singular values on the main diagonal, which are square roots of the nonzero eigenvalues of \\(AA^T\\) (or equivalently, of \\(A^TA\\)).
2. \\(U:\\>m\times m\\). Columns are \\(m\\) orthonormal eigenvectors of \\(AA^T\\). Indeed, \\[AA^T=U\Sigma \Sigma^TU^T\\] \\[\rightarrow (AA^T)U=U\Sigma \Sigma^T\\]
so that \\(U\\) is an eigenvector matrix of \\(AA^T\\), and the diagonals of \\(\Sigma \Sigma^T\in \mathbb\{R\}^\{m\times m\}\\) are eigenvalues of \\(AA^T\\).  
	* \\(AA^T\\) has full set of eigenvectors (\\(\textrm\{rank\}U=m\\)), since <a href="{{site.url}}/linear_algebra/2018/05/10/cross-prod-mat.html#normal_and_spectral" target="_blank">it is normal.</a>
2. \\(V:\\>n\times n\\). Columns are \\(n\\) orthonormal eigenvectors of \\(A^TA\\). Similarly, \\[A^TA=V\Sigma^T\Sigma V^T\\] \\[\rightarrow (A^TA)V=V\Sigma^T\Sigma\\]
so that \\(V\\) is an eigenvector matrix of \\(A^TA\\), and the diagonals of \\(\Sigma^T\Sigma\in \mathbb\{R\}^\{n\times n\}\\) are eigenvalues of \\(AA^T\\).  
	* \\(A^TA\\) has full set of eigenvectors (\\(\textrm\{rank\}V=n\\)), since <a href="{{site.url}}/linear_algebra/2018/05/10/cross-prod-mat.html#normal_and_spectral" target="_blank">it is normal.</a>  

Note that nonzero members of \\(\Sigma \Sigma^T\\) and \\(\Sigma^T \Sigma\\) are exactly same.


### Few helpful remarks
1. #### Do \\(AA^T\\) and \\(A^TA\\) share eigenvalues?  
Trivially, apply spectral decomposition and then transpose. Or in equation, \\[AA^Tx=\lambda x\quad\rightarrow\quad A^TA(A^Tx)=\lambda (A^Tx)\quad\rightarrow\quad(A^Tx,\lambda) \textrm\{ is an eigenpair of \}A^TA.\\] And vice versa.
2. #### \\(A^TA,\\>AA^T\\) are positive semidefinite matrices  
\\(\rightarrow\\) follows by the <a href="{{site.url}}/linear_algebra/2018/05/24/positive-definite-mat.html#pd_and_least_sq" target="_blank">statement here.</a> They are "semi"definite, since we did not assumed \\(r=m\\) or \\(r=n\\), respectively.  
3. #### \# of nonzero entries in \\(\Sigma \Sigma^T\\) = # of nonzero entries in \\(\Sigma^T \Sigma\\) = rank\\(A\\).  
	1. First equality can be confirmed by performing multiplciation \\(\Sigma \Sigma^T\\) and \\(\Sigma^T \Sigma\\).  
	2. Let \\(r\\)=\# of nonzero members in \\(\Sigma^T\Sigma \\). Since <a href="{{site.url}}/linear_algebra/2018/05/18/eigenpairs.html" target="_blank">orthonormal eigenvectors corresponding to 0-eigenvalues are an basis of nullspace</a>, last \\(n-r\\) eigenvectors of \\(V\\) are a basis of \\(\textrm\{null\}(A^TA)\\).   
	3. Since <a href="{{site.url}}/linear_algebra/2018/05/10/cross-prod-mat.html" target="_blank">\\(A^TA\\) and \\(A\\) share nullspace</a>, \\(\textrm\{null\}A\\) has \\(n-r\\) basis and \\(\textrm\{rank(\}A)=r\\).  
4. In `3`, we see that (where \\(r\\) is the number of nonzero entries in \\(\Sigma^T\Sigma \\) or \\(\Sigma\Sigma^T \\))
\\[\textrm\{orthogonal complements of last \}n-r \textrm\{ eigenvectors of \}V\\]
\\[= \textrm\{first \}r \textrm\{ eigenvectors of \}V\\]
are the basis of the rowspace of \\(A\\). Working with \\(AA^T\\) we obtain opposite result (first \\(r\\) eigenvectors). In summary,  
* First \\(r\\) columns of \\(U\\): basis of columnspace of \\(A\\)  
* last \\(m-r\\) columns of \\(U\\): basis of left nullspace of \\(A\\)  
* First \\(r\\) columns of \\(V\\): basis of rowspace of \\(A\\)  
* last \\(n-r\\) columns of \\(V\\): basis of nullspace of \\(A\\)  


<h3 id="const_proof_of_svd">Constructive proof of (the existence of) SVD</h3>
Let \\(A\in\mathbb\{C\}^\{m\times n\}\\). WLOG we see that \\(r=\textrm\{rank\}A\leq \textrm\{min\}(m,n)\\).  
Since \\(A^TA\in\mathbb\{C\}^\{n\times n\}\\) is normal (and thus positive semidefinite), we have \\(n\\) eigenpairs 
\\[(\sigma_1^2,...,\sigma_r^2,0,...,0),(v_1,...,v_n)\\]where \\(v_i\\)s are orthonormal. Then, 
\\[A^TAv_i=\sigma_i^2v_i\\]
\\[\rightarrow AA^T(Av_i)=\sigma_i^2(Av_i)\\]
so that \\(Av_i\\) is an eigenvector of \\(A^TA\\). For nonzero \\(\sigma_i^2\\), \\(Av_i\\)'s length can be computed by
\\[\\|Av_i\\|^2=v_i^TA^TAv_i=v_i^T(\sigma_i^2v_i)=\sigma_i^2.\\]
Therefore to obtain an length 1 eigenvector of \\(AA^T\\), we divide \\(Av_i\\) by \\(\sigma_i\\).
\\[u_i:=\frac\{Av_i\}\{\sigma_i\}\quad\rightarrow\quad Av_i=u_i\sigma_i\\]
\\[\rightarrow AV=U\Sigma\\]
\\[
\begin\{bmatrix\}\{\}
& & & \cr
& A & & \cr
& & & \cr
& & & \cr
& & &
\end\{bmatrix\}
\begin\{bmatrix\}\{\}
| & & & | \cr
v_1 &... &... & v_n \cr
| & & & | \cr
& & &
\end\{bmatrix\}
=
\begin\{bmatrix\}\{\}
 & & & & \cr
| & & & &| \cr
u_1 &... &... & ... & u_m \cr
| & & & & | \cr
 & & & & 
\end\{bmatrix\}
\begin\{bmatrix\}\{\}
\sigma_1& & & \cr
& \ddots & & \cr
& & \sigma_r & \cr
& & & 0 \cr
& & &
\end\{bmatrix\}
\\]
\\[(m\times n)\quad\quad\quad\quad(n\times n)\quad\quad\quad\quad\quad=\quad\quad\quad(m\times m\)\quad\quad\quad\quad(m\times n)\\]

We see that obtained \\(u_i\\)s are orthogonal to each other, thanks to the orthogonality of \\(v_i\\)s.
\\[u_j^Tu_i=(\frac\{1\}\{\sigma_j\}Av_j)^T(\frac\{1\}\{\sigma_i\}Av_i)=\frac\{1\}\{\sigma_j\sigma_i\}v_jA^TAv_i=0\\]
\\[u_i^Tu_i=(\frac\{1\}\{\sigma_i\}Av_i)^T(\frac\{1\}\{\sigma_i\}Av_i)=\frac\{1\}\{\sigma_i^2}v_iA^TAv_i=\frac\{\sigma_i^2\}\{\sigma_i^2}=1\\]


Next: Pseudoinverse