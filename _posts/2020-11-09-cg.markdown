---
layout: post
title:  "Conjugate Gradient"
date:   2020-11-09 09:00:05 +0800
categories: linear_algebra
use_math: true
tags: linear_algebra math need_review
---

* revision in 20.11.23

<a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" target="_blank">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a>

### Intro, Notation


- CG is effective for the systems of the form \\(Ax-b\\), where \\(A\\) is symmetric and positive definite
- for a quadratic system with such \\(A\\), let
  -  \\(f(x):= x^TAx-x^Tb+c\\)
  -  \\(f'(x)=Ax-b\\)
  - then it's hessian \\(A\\) is positive-definite
    - Then \\(f\\) is convex, and we can find \\(x^*\\) which minimizes \\(f\\) by solving \\(Ax-b=0\\)
- If \\(A\\) does not have such properties, CG (and steepest descent) will likely fail (by finding saddle point)
- The value of \\(b\\) and \\(c\\) determine where the minimum point of the paraboloid (=level set of quadratic func) lies, but do not affect the paraboloid's shape

### Steepest Descent
Let us define
* `error` \\(e\_i := x\_i - x^* \\), where \\(x^*\\) is a solution.
* `residual` \\(r\_i=b-Ax\_i\\)
  * It is easy to see that \\(r\_i=-Ae\_i\\), since \\(Ax^*=b\\) holds.
    * you should think of the residual as being the error transformed by \\(A\\) into the same space as \\(b\\)
    * at the same time, it is the negative gradient direction at \\(x\_i\\) (namely, \\(-f'(x\_i)\\))
* in general form, the next step will be \\[x\_\{i+1\}=x\_i+\alpha d\_i\\] In the case of steepest descent, with the `search direction` \\(d\_i\\).
  * with the next search direction \\(d\_i\\),
    * `next error` \\(e\_\{i+1\}=e\_i+\alpha d\_i,\quad\quad\\) since \\(e\_\{i+1\} = x\_\{i+1\}-x^* = x\_i+\alpha d\_i - x^* = e\_i+\alpha d\_i\\)
    * `next residual` \\(r\_\{i+1\} = r\_i-\alpha Ad\_i,\quad\quad\\) since \\(-Ae\_\{i+1\} = -A(e\_i+\alpha d\_i) = r\_i-\alpha Ad\_i\\)
* __Note that,__ \\(-r\_i = f'(x\_i)\\) in our problem setting
* In the _steepest descent,_ we will go down to the negative gradient direction (\\(f\\) is continuous function)
\\[d\_i=r\_i\\]
so that
\\[x\_\{i+1\} = x\_i+\alpha r\_i\\]


#### Line search

- Generally it is not easy to determine the stepsize, but with the condition above the optimal stepsize \\(\alpha\\) in Steepdest descent can be found by solving closed form problem
- Since \\(f\\) were convex, the composition function \\(f(x\_i+\alpha r\_i)\\) is also a convex function (convex function on a convex domain (line)). We set derivative to 0 so that
- \\(\frac\{\partial\}\{\partial \alpha\}f(x\_i+\alpha r\_i) = f'(x\_i+\alpha r\_i)^Tr\_i = f'(x\_\{i+1\})^Tr\_i = 0 \\)
- we see that, the search direction is orthogonal to the gradient (of the next point \\(x\_\{i+1\}\\))
- <img src="{{site.url}}/images/math/steepest_descent_alp.jpg" width="600">  
- \\(\alpha = \frac\{r^T\_ir\_i\}\{r^T\_iAr_i\}\\)
- The update will be  
 <img src="{{site.url}}/images/math/steepest_descent.jpg" width="1000">
- The equation (10) in the link is the steepest descent steps. We see that matrix-vector form is used, so it can easily utilize the sparsity


#### Convergence of Steepest Descent
1. Let's first consider where \\(e\_i = x\_i-x^*\\) is an eivengector of \\(A\\). The the residual \\(r\_i=-Ae\_i = \lambda\_ie\_i\\) is __also an eigenvector.__
Then equ12(whole update equation) gives \\(e\_\{i+1\}=0\\) by
\\[e\_\{i+1\} = e\_i+ \frac\{r^T\_ir\_i\}\{r^T\_iAr_i\}r\_i\\]
\\[ = e\_i+ \frac\{r^T\_ir\_i\}\{Ar^T\_ir\_i\}(-\lambda\_ee\_i)\\]
\\[ = e\_i - e\_i = 0\\]
1. If not, we can express \\(e\_i\\) as a linear combination of the __orthonormal eigenvectors__ of \\(A\\). (refer to the Equ 19-23).   
briefly, \\(e\_i=\sum\_\{j=1\}^n\xi\_jv\_j\\) where \\(v\_j\\)'s are orthogonal and let \\(\lambda\_j\\) be the corresponding eigenvalues. Then 
\\[r\_i = -Ae\_i = \sum\_\{j=1\}^n\xi\_j\lambda\_jv\_j\\]
\\[ r\_i^Tr\_i =  (\sum\_\{j=1\}^n\xi\_j\lambda\_jv\_j)(\sum\_\{j=1\}^n\xi\_j\lambda\_jv\_j) = \sum\_\{j=1\}^n\xi\_j^2\lambda\_j^2\\]
\\[r\_i^TAr\_i =  \sum\_\{j=1\}^n\xi\_j^2\lambda\_j^3\\]
so that
\\[e\_\{i+1\} = e\_i+ \frac\{r^T\_ir\_i\}\{r^T\_iAr_i\}r\_i\tag\{equ12\}\\]
\\[e\_\{i+1\}=e\_i+\frac\{ \Sigma\_j \xi\_j^2\lambda\_j^2 \}\{ \Sigma\_j\xi\_j^2\lambda\_j^3 \}r\_i\\] where \\(\xi\_i\\) are weight of the linear combination.  
> If all eigenvalues are same __(circular level set!),__ then \\(e\_\{i+1\}=0\\) !!
1. __Later, in CG, we are going to minimize a distance which measures the level set of__ \\(A\\) __as circular.__
2. 일반적인 케이스는 18p A-energy norm으로 분석가능 (condition number \\(k\\) 와 초기위치에 의해 결정되는 \\(\mu\\)에 의해 결정됨 (아래 식의 \\(\omega\\)는 \\(k, \mu\\)의 함수). \\(k\\)가 크거나, \\(k=\pm\mu\\)일시 수렴이 느림)
\\[\|\|e\_\{i+1\}\|\|^2_A = \|\|e\_i\|\|^2_A\omega^2\\]
6. upper bound 는 \\(k^2=\omega^2\\) (위의 조건)일 시,
\\[\omega \leq \frac\{k-1\}\{k+1\}\\]
... (refer to the paper)


### Conjugacy 

- preliminary : 우리의 convex quadratic 문제에서는 \\(e\_\{i+1\}^Td\_i=0\\)이 항상 성립해야 함
  - 
- `Idea` : pick a set of `orthongonal search direction` \\(\\{d\_0,...,d\_\{n-1\}\\}\\), and take exactly one good step to each directions (to gaurantee \\(n\\) step convergence)
  - 그렇게 골랐을 시, \\(e\_\{i\}\\)에서 \\(d\_i\\)에 orthogonal한 모든 성분을 뺀 것이 \\(e\_\{i+1\}\\)이기 때문에 (그람-슈미트), \\(e\_\{i+1\}^Td\_i\\)가 성립함
  - 그러나 이 경우, stepsize alpha를 계산할 수 없음 (epu30 참고)
- __instead__,  pick a set of `A-orthongonal search direction` \\(\\{d\_0,...,d\_\{n-1\}\\}\\), and take exactly one good step to each directions  
  - \\(e\_\{i\}\\) \\(d\_i\\) 에 A-orthogonal한 모든 성분을 뺸 것이  \\(e\_\{i+1\}\\)이기 때문에 (A-그람 슈미트), \\(e\_\{i+1\}^TAd\_i=0\\)이 성립
  - 이 equality는 `우연찮게도` 비교적 쉬운 convex 문제였던 line search를 arbitrary \\(d\_i\\)에 대해 풀었을 때의 optimality condition과 동일함. 
  - 이 경우 stepsize alpha가 계산가능해 짐. optimality condition부터 시작하면,    
    \\[-\frac\{\partial\}\{\partial \alpha\} f(x\_\{i+1\}) = 0\\]
    \\[f'(x\_\{i+1\})^Td\_i = 0\\]
    \\[-r\_\{i+t\}^Td\_i = 0\\]
    \\[d\_i^TA\_e\{i+1\} = 0\\] 이 되어, line search optimality condition \\(\rightarrow\\) A-orthogonality가 되고 
    \\[d\_i^TA(e\_i+\alpha\_id\_i)\\]
    \\[\alpha\_i = -\frac\{ d\_i^TAe\_i \}\{ d\_i^TAd\_i  \} = \frac\{ d\_i^Tr\_i \}\{ d\_i^TAd\_i  \} \tag\{31\}\\] 가 되므로 stepsize \\(\alpha\\)를 계산할 수 있음
- __Note that, if the search direction__ \\(d\_i\\) __were residuals, then the above is equivalent to performing steepest descent__
 
#### N step convergence
- express the error term \\(e\_0\\) as the linear combination of search direction \\(d\_i\\)s. \\[e\_0 = \sum\_\{j=0\}^\{n-1\} \delta\_jd\_j\\] Then by multiplying \\(d_k^TA\\) on both sides, we can calculate the weight (of the linear combination) \\(d\_k\\) by  
  <img src="{{site.url}}/images/math/steepest_descent2.jpg" width="1200">   
- in 4th line, we use __the assumption__ \\(d\_k^TAd\_j = 0 \text\{ for \} k\neq j\\)
- __notice that__, by combining equ31 and equ34, we find that \\(\alpha\_i=\delta\_i\\). This fact gives us a new way to look at \\(e\_i\\).
  - For each step, we cut down the error term component by component
    \\[e\_i = e\_0 + \sum\_\{j=0\}^\{i-1\}\alpha\_jd\_j\\]
    \\[= \sum\_\{j=0\}^\{n-1\}\delta\_jd\_j - \sum\_\{j=0\}^\{i-1\}\delta\_jd\_j\quad\quad \text\{stepsize \}\alpha\_j=-\delta\_j\\]
    \\[e\_i=\sum\_\{j=i\}^\{n-1\}\delta\_jd\_j\tag\{35\}\\] (epu29 refers to the update equation \\(x\_i+1=x\_i+\alpha d\_i\\))
  - after \\(n\\) iteration, every component is cut away, and \\(e\_n=0\\); the proof is complete

#### Gram-Schmidt Conjugation
- like we did in <a href="{{site.url}}/linear_algebra/2018/05/15/orthonormal-basis.html" target="_blank">Gram-Schmidt</a>, to construct A-orthogonal \\(d\_i\\) from arbitrary basis \\(u\_i\\)s, take \\(u\_i\\) and subtrack any components that are not A-orthogonal to the previous \\(d\\) vectors. i.e. \\[d\_i=u\_i + \sum\_\{k=0\}^\{i-1\}\beta\_\{ik\}d\_k\\] where \\(\beta\_\{ik\}\\) is defined for \\(k < i \\) (thus, N^2 for finding each \\(d\_i\\), which gives \\(O(n^3)\\)). 
- To calculate their values, use the same trick used to find \\(\delta\_j\\) (multiplying \\(d\_j^TA\\) to \\(d\_i\\) to find \\(\beta\_\{ij\}\\))  
  <img src="{{site.url}}/images/math/steepest_descent3.jpg" width="1200">   
- when we use \\(d\_i\\) as the standard unit basis, the process of Gram-Schmidt conjugation is equivalent to performing Gaussian elimination. 

> Remember that when one is performing the method of Conjugate Directions (including CG), one is simultaneously performing the method of Orthogonal Directions in a stretched (scaled) space

#### Optimality of the Error Term
Conjugate directions has an interesting property : Let \\(D\_i\\) be the i-dimensional subspace \\(\text\{span}\\{d\_0,...,d\_\{i-1\}\\}\\). The value \\(e\_i\\) is __chosen__ (by calculating \\(\alpha\_i\\), all the \\(\beta\_\{ik\}\\), and \\(d\_i\\)) from \\(e\_0+D\_i\\) in a manner which `minimizes the A-energy norm` \\(\|\|e\_i\|\|\_A\\). 

TO prove, using equ35,  
<img src="{{site.url}}/images/math/steepest_descent4.jpg" width="700">   
we see that, the summation only has not yet traversed search direction \\(d\_j, j>i\\). Any other \\(e\in e\_0+D\_i\\) must include previously traversed directions \\(d\_j, j < i \\) in its linear combination, so \\(e\_i\\) must have the minimum energy norm.
- 어떻게 매 단계 \\(i\\) 마다, 에러 벡터 \\(e\_i\\) 에서 A-orthogonal한 \\(d\_i\\) 컴포넌트를 지울 수 있는가? - 원래 문제가 컨벡스 문제이고, line search (convex function on convex doamin)도 컨벡스 문제이기 때문
  - 그렇다면 왜 steepest descent에서는 이런 좋은 현상이 나지 못했나?
    - search direction끼리 orthogonal하지 않았기 때문에? (현재 포인트 \\(x\_0\\)에 의해 좌우되는 \\(f'(x\_0)\\)) 
    - 왜 A-orthogonal한 search direction이어야 했나? - 
  
- We are minimizing A-energy norm - which is equivalent to changing the basis to the space where the level set of \\(A\\) is circular (not ellipsoidal) (to check this, do spectral decomposition \\(x^TQAQ^Tx\\) and put any orthonormal eigenvector \\(q\_i\\) to \\(x\\))= stretching the space!


- Another important property is that, \\(r\_i\\) is orthogonal to \\(D\_i\\). To show this, multiply \\(-d\_i^TA\\) to equ35
\\[-d\_i^TAe\_j = - \sum\_\{j=1\}^\{n-1\}\delta\_jd\_i^TAd\_j\tag\{38\}\\]
\\[-d\_i^Tr\_j=0 \quad j>i\quad\text\{by A-orthogonality of d-vectors\}\tag\{39\}\\]
  - remember that \\(r\_i=-Ae\_i\\) by out problem definiton (only applicable on good quadratic funcs!)
- because \\(d\_i\\)s are constructed from \\(u\\) vectors, \\(D\_i\\) also can be spanned by \\(\\{u\_0,...,u\_\{i-1\}\\}\\)


### Conjugate Gradient (finally!)
- in CG, the search directions are constructed by the conjugation of the residuals \\(u\_i=r\_i\\). We use residuals to construct the search direction.
- by equ39, we see that residual \\(r\_i\\) is orthogonal the previous search directions \\(d\_j, j<i\\).
- now equ41 becomes \\(r\_i^Tr\_j=0\\), so residual is orthogonal to the previous residuals - which implies independence


#### Krylov subspace
in `next residual` term \\[r\_\{i+1\}=r\_i-\alpha\_iAd\_i\\], we see that each new residual \\(r\_\{i+1\}\\) is a linear combination of previous residual \\(r\_i\\) and A\\(d\_i\\). 
- recalling that \\(r\_0=d\_0\\), and \\(d\_\{i-1\} \in D\_i\\) is a linear combination of previous residuals \\(r\_k, k<i-1\\), we see that  
  \\(D\_i\\) is formed from the union (in terms of span) of the previous subspace \\(D\_\{i-1\}\\) and \\(AD\_\{i-1\}\\). Using induction,
  \\[D\_i=span\\{r\_0,Ar\_0,...,A^\{i-1\}r\_0\\}\\]
  \\[D\_i=span\\{d\_0,Ad\_0,...,A^\{i-1\}d\_0\\}\\]
- this subspace is called a `Krylov subspace`
- convergence analysis 전까지 나머지는 걍 읽으면 됨. 정리할 시간이 없네 ㅡㅡㅋ precondition은 뺄까...