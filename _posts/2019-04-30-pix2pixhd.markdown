---
layout: post
title:  "(pix2pixHD) High-resolution Image Synthesis and Semantic Manipulation with Conditional GANs"
date:   2019-04-30 09:00:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning conditional_gan generative pix2pix
---

<a href="https://arxiv.org/abs/1711.11585" target="_blank">https://arxiv.org/abs/1711.11585</a>

* paired - attack point!
* multi-scale G
* GAN loss (lsgan) + percepual loss (feature matching loss?)
* multi-scale D (3)
* instance map \\(\rightarrow\\) boundary map and concat to input

### Abstract, Intro

* generate 2048 x 1024 visually appealing results with a novel adversarial loss (?), as well as new multi-scale G and D.
* extend framework to interactive visual manipulation with two additional features.  
	1. incorporated object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category.
	2. proposed a method to generate diverse results given the same input.
* new approach that produces high-res images from semantic label maps.
* baseline - pix2pix (GAN loss + conditional setting)
* Chen and Koltun [5]  
> suggest that adversarial training might be unstable and prone to failure for high-res image generation tasks. Instead, they adopt a modified perceptual loss to synthesize images, which are high-res but often lack fine details and realistic textures.

Here we __address two main issues__ of the above SOTA methods
1. difficulty of generating high-res imgs with GANs
2. the lack of details and realistic textures in the prev high-res results[5]

Contributions
1. adversarial loss + perceptual loss[5] (`VGG` feature mapping? are they different?)
2. used instance segmentation info (boundary) - enables flexible manipulations(?!), such as adding/removing objs and chaning types
3. proposed a method to generate diverse results given the same input label map
4. Compared against SOTA results [5, 21] in terms of __quantitive / human perception__
5. Ablation study on training objs/using instance lvl seg info.
6. edge2photo application (generalizability of approach)


### Conclusions
* c-GANs can synthesize high-res photo-realistic imgs without any hand-crafted losses or pre-trained nets.
* incorporating a perceptual loss [22] slightly improved the result
* useful for area (medical imging, and biology) where pretraining is not available
* img2img synthesis pipeline can be extended to produce diverse outputs

### Related Works


<h4>Image-to-image translation</h4> \\(\quad\\) Compared to \\(L\_1\\) loss, which often leads to blurry images [21, 22], the adversarial loss [16] has become a popular chice for many image-to-image tasks. The reason is that the __D can learn a trainable loss function__ and automatically adapt to the differences between the G and real iamges in the target domain. (ex-pix2pix)

#### [5] 
\\(\quad\\) made new GAN loss based on [5]. side by side comparison.

#### Deep visual manipulation
\\(\quad\\) we focus on object-level semantic editing (how?!)

### The pix2pix baseline
supervised - works on pairs of images \\(\\{s\_i,x\_i\\}\\), where
* \\(s\_i\\): semantic map
* \\(x\_i\\): corresponding natural photo

Aim to model the conditional dist. of rela images, given the input semantic label maps via
\\[\min\_G\max\_DL\_\{GAN\}(G,D)\\]
\\[L\_\{GAN\}(G,D):= E\_\{s,x\}[\log D(s,x)] + E\_s[\log (1-D(s,G(s))]\\]
* Unet` as generator (with residual connection)
* Patch-based CNN as D (Patch-GAN)
* Cityscapes - 256 X 256
* _directly applying pix2pix on high-res - training unstable + images unsatisfactory_

### G architecture
<img src="{{site.url}}/images/deeplearning/gan/g_pix2pixhd.png" width="1000"/>  
= local enhancer + Global G + local enhancer (Notice the residual connection in the image)
* `Global G` : Resnet G of Johnson (Conv - Resblock - TConv)
* `Local enhancer` : same structure 
* trained global, local, then fine tune jointly  
The paper says, _such multi-res pipeline is a well established practice in CV[4]_ and _two scale is often enough[3]_. In GANs, [9, 19] - unconditional, [5, 57] - conditional

### D architecture
Single D with large receptive field \\(\rightarrow\\) costs too much memory \\(\rightarrow\\) 3 multiscale D with same architecture?  
What about D with aux head?
\\[\min\_G\max\_\{D\_1,D\_2,D\_3\}\sum\_\{i=1\}^3L\_\{GAN\}(G,D\_i)\\]

### Improved adversarial loss (feature matching)
the loss
* is a feature mathching loss based on the D.
* __stabilizes training as the generator has to produce naturla statistics at multiple scales__ (isn't this too strong regularization?)

With \\(D\_k^\{(i)\}\\) : \\(i\\)th layer of \\(D\_k\\),
\\[L\_\{FM\}(G,D\_k)=E\_\{(s,x)\}\sum\_\{i=1\}^T\frac\{1\}\{N\}\|\| D\_k^\{(i)\}(s,x)-D\_k^\{(i)\}(s,G(s))\|\| \\]
where
* \\(T\\): Total # of layers (seriously?)
* \\(N\_i\\): # of elements in each layer


* Related to the perceptual loss [11, 13, 22] - useful for super res & style transfer
* ablation study?

### Instance map
> ... we argue that the most critical info the instance map provides, which is not available in the semantic label map, is the obj bouundary.

- implementation details of how they extracted boundary map from inst img is written

### Learning an Instance-level Feature Embedding
> Image synthesis from semantic label maps is a one-to-many mapping problem.

* must generate diverse images from one semantic map
* [5, 15] produced fixed # of discrete outputs given the same input
* [66] synthesize diverse modes controlled by a latent code that encodes the entire image
> No instance-level control on the generated contents

1. To generate diverse imgs and allow instance lvl control, we propose adding additional low-dimensional feature channels as the input to the generating network
2. By manipulating these features, can have flexible control over the img synthesis process
3. train feature encoder Network \\(E\\) (standard encoder-decoder) to find low-dimensional feature vector that corresponds to the GT target for each instance in the image
4. To ensure the features are consistent within each instance, we add an instance-wise average pooling layer to the output of the encoder to compute the avg feature for the obj instance. The avg feature isthen broadcast to all the pixel locations of the instance.
5. Replace \\(G(s)\\) with \\(G(s, E(x))\\), train the encoder jointly with \\(G, D\\).
6. After the \\(E\\) is trained, run \\(E\\) over all instances in the training imgs and record the obtained features
7. Perform a K-means clustering on features for each semantic category. Each cluster thus encodes the features for a specific style, EX - asphalt or cobblestone texture for a road
8. At inference, randomly pick one of the clusrer centers and use it as the encoded feature.
9. These features are concated with segmap and used as the input to \\(G\\).


### Quantitive comparsion
Ran pretrained `PSPNet` on generated images. MIOU 0.6389 (Prescan was like 30?)

### Human perception study
meh





Next - Chen and Koltun [5] (Photographic image synthesis with cascaded refinement networks)