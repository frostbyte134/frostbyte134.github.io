---
layout: post
title:  "Xception: Deep Learning with Depthwise Separable Convolutions"
date:   2018-09-24 09:00:05 +0800
categories: deep_learning
use_math: true
tags: deep_learning xception depthwise_separable_conv inception imagenet jft
---

<a href="https://arxiv.org/abs/1610.02357" target="_blank">https://arxiv.org/abs/1610.02357</a> 

Note that, in pytorch, separable conv is done with the named params group=in_channel.

### Abstract
* Convolution in 1-D discrete spectrum  
Regular \\(\quad \Leftrightarrow \quad\\) Inception \\(\quad\Leftrightarrow\quad\\) Depthwise (3by3, spatial) and separable (1by1, channel)  
The paper changes Incpetion module to depthwise separable ones in the extreme (from `VGG`), thus Xception.
* Same params with `Incep-V3`, but slightly better \\(\rightarrow\\) good architecture
* In short, the Xception arthitecture is a linear stack of depthwise separable convolution layers with residual connections.

### The Inception Hypothesis
A conv layer attemps to learn filters in a 3D space, by simultaneously mapping cross-channel (1D) and spatial (2D) correlations.

> The idea behind inception is, to make this proc easier and efficient by explicitly factorizing it. That would __independently__ look at cross-chyannel / spatial correlations. 

> In effect, the fundamental hypothesis behind Inception is that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly.

<img src="{{ site.url }}/images/deeplearning/xception.png" class="center" style="width:1000px"/>  

* The Inception module first tries to find cross-channel correlation, then look for the best spatial correlation.
* We remark that this extreme form of an Inception module is almost identical to a `depthwise separable convolution`.

Two minor differences between Depthwise separable and "extreme" Inception module would be:  
1. The order of operation  
	* depthwise separable: spatial(3by3) - channel(1by1)
	* extreme exception: channel(1by1) - spatial(3by3)
2. The presence / absence of non-linearity after the first operation.
	* depthwise separable: non non-linearity in the middle
	* extreme exception: uses ReLU (not in this paper, but in previous Inception papers)

	
### The Xception architecture
> In short, the Xception arthitecture is a linear stack of depthwise separable convolution layers with residual connections.

<img src="{{ site.url }}/images/deeplearning/xception2.png" class="center" style="width:1000px"/>  

### Experimental Evaluation

* Trained on 60 K80, Tensorflow
* Imagenet: sync `SGD` with 0.9 / 0.045, 094 for 2epoch
* JFT: async RMSProp with 0.9, 0.001, 0.9 for 3000000 samples
* JFT took 1month, Imagenet took 3days each (WTF?)
* __Dropout was removed on JFT,__, since the large size of the dataset which made overfitting unlikely in any reasonable cmout of time.

* Inserting ReLU on the middle gave bad performance, in convergence / validation acc.
	1. This is the opposite result with the `VGG` paper.
	2. > It may be that the depth of the intermediate feature spaces on which spatial convolutions are applied is critical to the usefulness of the non-linearity:  
	for deep feature spaces (e.g. Inception on `VGG`), the non-linearity is helpful,  
	but for shallow ones (e.g. the 1-channel deep feature spaces of depthwise separable convs) it cebomes harmful, possibly due to a __loss of information__.


Next:  