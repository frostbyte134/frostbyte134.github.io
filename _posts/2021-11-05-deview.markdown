---
layout: post
title:  "Deview"
date:   2021-11-10 08:00:05 +0800
categories: coding
use_math: true
tags: coding
---


### 손쉽게 ML 라이프사이클을 다룰 수 있는 MLOps (2020)
- <a href="https://tv.naver.com/v/16970565" target="_blank">https://tv.naver.com/v/16970565</a>
- AI연구자(피처추출, 트레이닝, eval)와 엔지니어 영역 (HA, 배포, 검증, scale)이 다름
- 엔지니어
 - 자원낭비가 신경쓰임 (트래픽 예측 어려움, 많은 GPU를 확보하고 시작해야 함)
 - 인퍼런스를 하기위한 최소한의 장비사향을 잘 모름
 - 모델 표준화: 이상적인 상황에선 엔지니어와 AI연구자 분리 가능
    - SavedModel (TF), ONNX (Pytorch), TensorRT (인퍼런스 최적화, Trition Inference 서버로 서빙가능?)
- MLFLOW models
    - 딥러닝 이외에도 여러 ML 프레임워크 모델 사용 가능
    - 현재 네이버에선 SavedModel, ONNX, TensorRT만 지원하면 충분하다고 생각중. MLFlow는 추가코드가 필요해서 안한다고 함 (AI연구자들에게 요구 추가 필요)
- Model Registry
    - 모델과 부가정보 (MySQL?) 를 저장. 실제 모델파일은 HDFS에.
    - mysql: 모델 uri, hyperparams, desc (어떤데이터), metrics, id
    - 연구자와 엔지니어 사이의 도구
    - ML-metadata를 사용? kubeflow에서도 사용?
        - context, artifact? ml에서 데이터를 하는 것을 잘 표현??? 뭔진 잘 모르겠음
    - 모델 (id)을 선택하면 인퍼런스 서버를 실행할 수 있음 (스타게 비슷한 거인듯). AISP라는 플랫폼이 `인퍼런스 매니저`라는 걸로 실행한다고 함
    - 로드밸런싱, 에러처리 등도 필요. 
    - 모델id로 버전관리하기 위해 model registry에서 이름별로 버전, 배포 관리 등
- model valicdation
    - 설계검토만 했다고 함
    - validation목적: inference결과가 이상하게 나오는 것을 방지
    - 연구자의 개입없이 - production vs stage? 흠.
- 기타: CPU inference
    - 모니터링, 성능테스트 (자원 모니터링) - 서서히 부하를 올리면서 부하 수집
- 생각해보면 fluentd로 모았다가 카프카로 쏘는 방법이 굉장히 좋은 듯 (latency가 거의 없으니까.)
- MPS: GPU를 나눠서 사용하며 성능저하 최소화 (Pascal 이후로)
    - 메모리 사용률 제한은 불가 (양심에 맡겨야 한다고 함)
- CPU성능테스트
    - cpu몇개를 쓸떄 효율적인지 테스트해봐야 함. `OMP_NUM_THREADS`? openmp 아닌가 이거 스타포트 어디서 본거같은데
- 오토스케일링 - ㄷㄷ 이거 쓰나

### 밑바닥부터 만드는 AI서빙 플랫폼
- https://tv.naver.com/v/16968271
#### 모델서빙 공통화 
- 동기: 모델수의 증가, 지속적인 학습 및 배포, 커스텀한 사내시스템 연동 (제공되는 사내시스템이 많아서 연구자들이 연동하는게 쉽지 않음), 협업가능한 (모델러 / 엔지니어 관심의 영역 분리, 효율성 추구) 환경 구축
- 구성요소
    - gateway: 인증, 로깅, 트래픽 관리, 비동기 처리 등을 게이트웨이로 넣음. 엔진엑스도 사용가능하지만 사내시스템과 연동도 해야 해서 직접 개발? 
    - model sdk: 모델러와 협업을 할 수 있는 인터페이스를 정의, GRPC등의 라이브러리 제공, 효율적으로 코드의 중복 없이 개발. 모델 디스커버리, 모델 로깅, 헬스체크? 게이트웨이와 연계로직이 들어감.
    - deploy: 배포용 시스템. 이종 시스템이 많기 떄문에 추상화된 인터페이스를 제공 (이거도 중요한 포인트인듯)

#### Gateway
- MDS: 서빙플랫폼의 모델 패키징? 모델 배포
- 사내에서의 모델서빙 요구사항
    - 다양한 ML프레임웍 (Pytorch, Tensorflow), RESTAPI, GPU, Docker (모델 종속성, 환경 해결). 게이트웨이에서 이를 다 고려해야 했음
    - 프레임웍별로 base이미지 / Rest API (Decorator, Flask) / GPU용 오퍼레이터 구현, 배처치리 추가 / 모든배포는 도커로
- 보통 요청 - 전처리 - 인퍼런스 - 후처리 - 응답
    - 각 단계를 핸들러로 구현할 수 있는 데코레이터를 구현 (함수 만들고 위에 `@mds.inference` 이런 식)
- 모델패키지 (도커이미지)를 모델러가 핸들러를 이용해서 만들면 이를 배포시스템에 태워주는 거인듯
- 배치인퍼런스: MDS에서 request를 모아서 n개를 한번에 요청 (미들웨어인듯)
- 멀티모델 파이프라인: 여러 모델이 연결되어 서빙될 시 (모델 A - 모델 B)
    - 약간 젠킨스 파이프라인처럼...? yaml파일 작성해야 하는 듯

#### DPLO
- 모델 배포 서버
- CPU/GPU 클러스터에 배포 지원 (오퍼레이터? k8s오퍼레이터 말하는건 아니지?)
    - 선언적 배포구성 파일 도입, 엔드포인트 자동등록 (이거 걍 k8s아닌가)
- 여기서 MDS패키지 (도커이미지?)를 다운받아서 요청받은 클러스터로 배포함 (argoCD같은 느낌?)
    - 배포 완료하면 엔드포인트 체크해서 라우팅 추가
- k8s랑 C3DL (?) 오퍼레이터를 썼다고 함
- 고랭 사용: 클러스터 배포 시 연동이 비교적 쉽다고 함 (k8s, docker와 연동이 쉬움)
- api는 grpc + rest api
- 배포시 설정 json파일 제공: 트래픽 버킷팅도 제공해주는듯
- 선언파일을 통한 배포: git을 통해 배포제어 관리 가능
- 동적 endpoint discovery: k8s도 제공하지만 모든 클러스터가 이걸 제공하지 않아서 (?) 그럼 전부 k8s쓰면 되는 거 아닌가. 난 근데 이부분 별 관심이 없어서...

### AI Gateway
- 라우팅 환경 요구사항: 라우팅, 배포
- 라우팅 스킴: 버전별로 (?) 로드밸런싱이 가능하게 했다고 함.
    - 버전별로 5:5 받고, 각 버전별로 내부적으로 또 쪼개기 가능?
- 동적 url 매칭 라우터: 고랭으로 구현. k8s ingress-nginx 쓰면 되는 거 아닌가 ㄷㄷ
- 요청 미러링: 이거도 엔진엑스에서 해 주는데...?
    - 성능비교 말씀하시는데, 응답에 대한 피드백은 못받지않나. 서빙타임 시간 같은거만 비교가능하지않나
- 가중치 기반 라우팅
    - 모델 배포가 어떻게 되어있는지에 대해 의존적으로 구현해야 하는듯
- AI로그에 대한 통일된 스키마 적용 가능
    - 이걸로 anormaly detection (정성적 (컨피던스) / 정량적 (500에러)). 나도 anormaly detection 같은거 만들어야 하는데...지금은 너무 수동임
    - 사내 시스템인 LOGISS 활용
    - 로그 시각화: 스샷보니 키바나임 ㅋㅋ ES에 필드 넣은걸로 구분가능하다 말씀하시는 듯
    - 모델 서빙 모니터링: 스샷보니 프로메테우스임
- recap & conclusion
    - 모델 패키징 및 배포 효율화
    - 선언파일을 통한 배포, 동적 엔드포인트 디스커버리
    - 서빙 플랫폼의 가치 - 단순 서버를 쉽게 띄워주는 게 아닌, 전체 모델 서빙 process의 효율화 라고 함.

### 실리콘밸리의 MLOps
https://velog.io/@xcellentbird/Review-%EC%8B%A4%EB%A6%AC%EC%BD%98%EB%B0%B8%EB%A6%AC%EC%9D%98-MLOps


### Motivation - Why did I start MLOps?
출처: https://deepinsight.tistory.com/179 [Steve-Lee's Deep Insight]



### 나를 따라하는 아바타 모델 개발부터 모바일에 적용하기까지
https://deview.kr/2020/sessions/395

### Few-shot handwriting copycat AI
https://tv.naver.com/v/16969155
- 한글이 글자 가짓수가 많아 폰트인식이 어렵다는 듯
- 인공지능으로 한글폰트 개발.....?
    - 256자로 폰트 개발, 작년에 한글화 손글씨 이벤트를 했다 함
    - OCR 팀이라고 하는데? (나 대신 손글씨 써주는 AI만들기, 이바도) - 모델 외 내용도 있다 함
- 모델: pix2pix (img2img translation, 이미지의 내용을 유지하면서 다른 스타일로 바꾸는 모델)
    - 낮 사진을 밤 사진으로
    - 컨텐츠를 유지하면서 바꿔 줌
    - 폰트생성도 똑같은 문제라고 함. 근데 스타일이라는게 다르지 않나 (이미지의 조도 같은거). 컨텐츠는 같다 치고... 
- 모델은 같은 구조라고 함 (인코더-디코더)
    - 중간에 스타일 레이블 삽입
    - 미리 pretrain, 256개만 이용해서 finetune (30분이 걸린다고?)
    - 만약 사람이라면 256자만 필요한가?
- 사람같이 적은 데이터로 하는 경우: few shots (수개의 데이터만으로도)
    - few shot font generation
    - 더 작은 자원으로 빠르게 생성가능. few shot vs many shot
    - many shot: 입력 - 인코더 - (스타일 레이블) - 디코더
    - few shot: 입력 - 인코더 - (스타일 레이블 + 스타일 피처 (from 스타일 인코더)) - 디코더
- 근데 기존 모델들은 잘 안됐음 (가, 나에서 최를 어떻게 만들까?)
    - 모델도 잘 모르겠어서 트레이닝 데이터를 외워버림 (오버피팅?)
    - 최를 만들고 싶으면 ㅊ을 미리 보여줘야 한다.
        - 28글자로 한글폰트 만들기
    - 근데 찬의 ㅊ과 체의 ㅊ이 같다고 인식시켜야 함 (잘 가이드를 해야 함)
        - 거 + 아 + 흠 -> 감
        - 모델이 외워버리니까 외우지 않아도 되게 문제를 내자 
        - 컨텐츠 인코더 제거 (조합성을 이해하는 것 보단 컨텐츠를 그냥 모델링하는게 트레이닝 로스를 떨어트리기 쉬워서)
- 인코더 - 초중종성 분리 - 다이나믹 메모리 (? 메모리라니까 뭔가 lstm 같은데 행렬인 듯) - 결과 초중종성 - 디코더가 합쳐 줌
- 갑분 matrix factorization - 메모리가 행렬인데 빈공간이 좀 있었음. 이걸 스타일벡터와 컴포넌트벡터로 분해해서 빈칸을 채웠다고 함
    - 대신 행렬분해는 보통 수치적으로 푸니까 비쌈
    - 모델이 factorize할 행렬을 예측하도록 바꿨다고 함. 이걸로 4글자로 만들 수 있었다고 함 ㄷㄷ