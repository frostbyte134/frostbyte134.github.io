---
layout: post
title:  "TCP socket + C10K Problem"
date:   2020-07-24 09:00:05 +0800
categories: problem_solving
use_math: true
tags: problem_solving need_review coding
---

유용한 링크들
- <a href="https://github.com/nailbrainz/c10k_test" target="_blank">github sources link</a>  
- <a href="https://d2.naver.com/helloworld/47667" target="_blank">TCP-IP의 근본</a>
- <a href="http://docs.likejazz.com/time-wait/" target="_blank">엄청 잘쓴 블로그</a> 여기 코드 좀 참고할 만 한듯
- <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/" target="_blank">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a>
- <a href="https://www.cs.dartmouth.edu/~sergey/netreads/path-of-packet/Network_stack.pdf" target="_blank">PATH OF A PACKET IN THE LINUX KERNEL STACK</a> <a href="https://pigbrain.github.io/network/2016/05/29/PathOfAPacketInTheLinuxKernelStack_on_Network" target="_blank">(한국어)</a>
- <a href="https://www.oreilly.com/library/view/linux-device-drivers/0596000081/ch05s02.html" target="_blank">blocking read, kernel side</a>
- <a href="https://evan-moon.github.io/2019/11/22/tcp-flow-control-error-control/" target="_blank">컨제스쳔 컨트롤, 에러컨트롤 대박 사이트</a>
- <a href="https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/" target="_blank">history and brokeness of select, epoll</a>
- <a href="https://uwsgi-docs.readthedocs.io/en/latest/articles/SerializingAccept.html" target="_blank">Thundering herds problem</a>
  - <a href="https://lwn.net/Articles/632590/" target="_blank">epoll thundering herd patch</a>

implementation of epoll
- <a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/" target="_blank">select, epoll의 구현. 이거만 잘 읽자</a>
  - `fs/eventpoll.c`
  - `net/ipv4/tcp.c`
  - 결국, socket filesystem에 `wait_queue`를 하나 등록시키고, 여기서 이벤트 기반으로 변화가 있을시마다 콜백함수가 호출되어 epoll구조체에 정보를 전닳해 줌. wait_queue기반의 이벤트라는것이 중요한 듯. 아직 polling 공부를 못해서...
- https://programming.vip/docs/linux-kernel-notes-epoll-implementation-principle.html
- optional : https://fd3kyt.github.io/posts/implementation-of-epoll/#fnref:fn-3

위의 것 prerequisite : wait_queue
1. <a href="https://www.kernel.org/doc/htmldocs/kernel-hacking/queues.html" target="_blank">커널 doc 설명</a>
   1. 파일시스템을 예로 들면, 각 파일마다 wait해야 하는 이벤트가 있다고 하자. 그럼 각 fd마다 `wait_queue_head_t`를 우선 선언함
   2. 이 파일시스템을 이용하는 proc중, 이 이벤트를 기다리고 싶은 프로세스는, `wait_queue_t` 를 선언하여 헤드에 붙이고 자신의 실행을 양보해서 sleep에 들어감 (인터럽티블 - `wait_event_interruptible`, 언인터럽티블 - `wait_event` 함수를 이용)
   3. 특정 조건이 만족되면, 다른 프로세스가 `wake_up()`을 호출함 - `TASK_EXCLUSIVE` 가 세팅되어 있으면 1개, 아니면 대기 큐에 있는 모든 프로세스가 TASK_RUNNING으로 전환되어 스케쥴링에 참여

States  
<img src="https://upload.wikimedia.org/wikipedia/en/5/57/Tcp_state_diagram.png" width="600" class="center"/>  


계획
1. select 기반 - 해봐야 1000개밖에 안될테지만..
    - 링크
2. epoll
3. 커널 컴파일 - 제로 카피?
4. nginx
5. TODO
   1. <a href="https://gist.github.com/Alexey-N-Chernyshov/4634731" target="_blank">https://gist.github.com/Alexey-N-Chernyshov/4634731</a> : catch sigint, sigterm and terminate gracefully

이슈들
- select최대는 1000개 컴파일 타임에 변경도 못한다 함
- socket생성 - bind (app과 포트번호 바인딩) - listen (server socket 활성화) 까지는 전부 동일하므로, 그 이후를 함수포인터로 추상화해서 메인의 중복을 줄임



### 공부할 것
- https://recipes4dev.tistory.com/153 TCP IP conn process. must review
- https://m.blog.naver.com/PostView.nhn?blogId=rev7707&logNo=10005157701&proxyReferer=https:%2F%2Fwww.google.com%2F 클라이언트는 포트 번호를 특정한 값으로 지정할 필요가 없다 (클라이언트에서는 보통 시스템이 자동적으로 배정하는 포트 번호를 사용한다.).
- why select is N^2? https://stackoverflow.com/questions/19282784/why-is-poll-better-than-select Many programs use other approaches. For instance sshd, the SSH daemon, spawns a child process for each connection. Others handle each connection in a thread.
- http://www.kegel.com/c10k.html#nb from the basics
- http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html kernel matters
- https://github.com/trptcolin/c10k I like the code
- https://stackoverflow.com/questions/10538291/what-is-the-point-of-noreturn attribute noreturn
- https://github.com/mrkschan/c10k-experiment python c10k
- https://www.slideshare.net/igalarzab/communication-in-python-and-the-c10k-problem comminication in python and c10k problem
- http://youngrok.com/asyncio%EC%97%90%20%EB%8C%80%ED%95%9C%20%ED%9A%8C%EC%9D%98 asyncio에 대한 회의
- https://github.com/aaugustin/django-c10k-demo



### TCP Socket
- 소켓의 식별자 = (source ip, source port, dest ip, dest port)

#### basics
> <a href="https://d2.naver.com/helloworld/47667" target="_blank">https://d2.naver.com/helloworld/47667</a>

- `Flow control` : 송신자는 수신자가 받을 수 있는 만큼 데이터를 전송한다. 수신자가 자신이 받을 수 있는 바이트 수 (사용하지 않은 버퍼 크기, receive window)를 송신자에게 전달한다. 송신자는 수신자 receive window가 허용하는 바이트 수만큼 데이터를 전송한다.
- `Congestion control` : 네트워크 정체를 방지하기 위해 receive window와 별도로 congestion window를 사용하는데 이는 네트워크에 유입되는 데이터양을 제한하기 위해서이다. Receive window와 마찬가지로 congestion window가 허용하는 바이트 수만큼 데이터를 전송하며 여기에는 TCP Vegas, Westwood, BIC, CUBIC 등 다양한 알고리즘이 있다. __Flow control과 달리 송신자가 단독으로 구현한다.__
- 여러 레이어가 있지만, 크게 유저(user) 영역, 커널(kernel) 영역, 디바이스로(device) 영역으로 나눌 수 있다. 유저 영역과 커널 영역에서의 작업은 CPU가 수행한다. __이 유저 영역과 커널 영역은 디바이스 영역과 구별하기 위해 호스트(host)라고 부른다__
- 현재 TCP 상태가 데이터 전송을 허용하면 새로운 `TCP segment, 즉 패킷`을 생성한다. Flow control 같은 이유로 데이터 전송이 불가능하면 시스템 콜은 여기서 끝나고, 유저 모드로 돌아간다(즉, 애플리케이션으로 제어권이 넘어간다).
- TCP segment에는 TCP 헤더와 페이로드(payload)가 있다. 페이로드에는 ACK를 받지 않은 send socket buffer에 있는 데이터가 담겨 있다. __페이로드의 최대 길이는 receive window, congestion window, MSS(Maximum Segment Size) 중 최대 값이다.__
- NIC가 패킷을 전송할 때 NIC는 호스트 CPU에 인터럽트(interrupt)를 발생시킨다. 모든 인터럽트에는 인터럽트 번호가 있으며, 운영체제는 이 번호를 이용하여 이 인터럽트를 처리할 수 있는 적합한 드라이버를 찾는다. 드라이버는 인터럽트를 처리할 수 있는 함수(인터럽트 핸들러)를 드라이브가 가동되었을 때 운영체제에 등록해둔다. 운영체제가 핸들러를 호출하고, 핸들러는 전송된 패킷을 운영체제에 반환한다.
- 애플리케이션이 쓰기 요청을 직접적으로 하지 않아도 커널이 TCP를 호출해서 패킷을 전송하는 경우가 있다. 예를 들어 ACK을 받아 receive window가 늘어나면 socket buffer에 남아있는 데이터를 포함한 TCP segment를 생성하여 상대편에 전송한다.
- NIC가 패킷을 받았는데, 드라이버가 미리 할당해 놓은 호스트 메모리 버퍼가 없으면 NIC가 패킷을 버릴 수 있다 (packet drop).

#### Layers
- `Ethernet layer` : 2계층과 관련있음 (data link). ARP, MAC주소.
  - Ethernet 레이어는 ARP(Address Resolution Protocol)를 사용해서 next hop IP의 MAC 주소를 찾는다. 그리고 Ethernet 헤더를 패킷에 추가한다. Ethernet 헤더까지 붙으면 (전송 시) 호스트의 패킷은 완성이다.
- `IP layer` : 3계층
  - 생성된 TCP segment는 IP 레이어로 이동한다(내려 간다). IP 레이어에서는 TCP segment에 IP 헤더를 추가하고, `IP routing`을 한다. 
  - `IP routing`이란 목적지 IP 주소(destination IP)로 가기 위한 다음 장비의 IP 주소(next hop IP)를 찾는 과정을 말한다. IP 레이어에서 IP 헤더 checksum을 계산하여 덧붙인 후, Ethernet 레이어로 데이터를 보낸다.
  - IP routing을 하면 그 결과물로 next hop IP와 해당 IP로 패킷 전송할 때 사용하는 인터페이스(transmit interface, 혹은 NIC)를 알게 된다.


establish, closing
- 당연한 말이지만 연결시 클라이언트가 무조건 먼저 걸음 (syn - syn+ack - ack)
- 종료시에는 서버/클라이언트 둘 다 active closer가 될 수 있음 (fin - ack, fin - ack)

#### TIME_WAIT
- active closer측에서 fin을 받고 (4-way에서 3번째) ack를 보낸 뒤 (4번째), 잠시동안 소켓을 유지하는 기간
- <a href="http://docs.likejazz.com/time-wait/" target="_blank">참고 링크</a>
- 왜 있나?
  - 지연된 데이터 패킷 대응 (FIN - ACK, FIN - ACK 정상 종료 후 수신되는 데이터 패킷)
  - active closer가 보내는 마지막 (4-way에서 4th) ack유실시, 상대방 (passive closer)은 `LAST-ACK`상태를 유지하고 (확인 필요) FIN 을 다시 보냄. 이 때 time_wait이 짧으면 active closer는 이미 자체적으로 소켓을 회수한 상태라, 유실된 ACK를 다시 보내주지 않고 RST를 보내게 됨
    - 결국 __Active closer는 자기가 보낸 마지막 ACK가 잘 갔는지 말았는지 모르기 때문에, 일정기간 기다려 줘야 하는 것__

소켓 생성/삭제가 잦은 경우, TIME_WAIT때문에 file descriptor (이건 좀...기본제한이 커서)/로컬 포트가 부족해 질 수 있다 함. ex) 서버는 DB 등에 대해 클라이언트가 되기도
- 클라이언트 : 항상 연결을 거는 입장이므로, 걸기 전 connection pool을 활용 (세션의 cache)
  - `net.ipv4.tcp_tw_reuse` : 로컬 포트 고갈 시 TIME_WAIT상태에 있는 소켓을 빨리 회수해 재사용 가능하게 해 줌
- `서버는 소켓을 열어 놓고 요청을 받는 입장이기 때문에 로컬포트고갈이 일어나지 않는다`
  - 위에도 있다시피, 소켓의 identifier 중 source port가 고정됨. connection은 여러 개 열지만, 다 같은 (source ip, source port) 로 여는 것 
  - `net.ipv4.tcp_tw_recycle` : TIME_WAIT값을 고정된 값이 아니라, RTO기반의 짧은 값으로 바꿔 줌. 
  - 또는 서버측에서 nginx / http keepalive를 세팅하면, GET등의 요청 처리 후 소켓을 잠시 살려 둠

예시
- 웹 --- nginx / 아파치 (웹 서버) --- tomcat (앱 서버) 에서, 웹서버-앱서버 간 `keepalive`를 적용하지 않으면 `TIME_WAIT` 발생
  - 소켓 고갈 (클라이언트=웹서버)은 tw_reuse로 어느정도 해결가능
  - 불필요한 3,4 way handshakes - 성능 낭비 발생
  - 웹 서버를 두는 이유 = HTTPS를 사용시, 인증서 설정/관리, <a href="https://www.popit.kr/%EB%82%B4-%EC%84%9C%EB%B2%84%EC%97%90%EB%8A%94-%EB%88%84%EA%B0%80-%EB%93%A4%EC%96%B4%EC%98%A4%EB%8A%94%EA%B1%B8%EA%B9%8C-%EC%8B%A4%EC%8B%9C%EA%B0%84-user-agent-%EB%B6%84%EC%84%9D%EA%B8%B0/" target="_blank">UserAgent</a> 확인, <a href="https://damedame.tistory.com/entry/HTTP-referer" target="_blank">HTTP Referer확인</a> 등 서비스 외적인 요소가 많기 때문에, __서비스 외적 (웹서버) / 서비스 (앱서버)__ 분리용
    - <a href="https://ko.wikipedia.org/wiki/HTTP_%EB%A6%AC%ED%8D%BC%EB%9F%AC" target="_blank">위키 리퍼러 페이지</a>


#### MISC
- `sudo sysctl -w net.core.somaxconn="2048"` : 무슨 효과가 있는거지? 엄청 낮게 설정해도 별거 없음
- 

### 파일 디스크립터 갯수 늘리기
내 C10K 첫번째 이슈였음. soft limit이 1024여서 930개 정도까지밖에 안열림. 근데 생각해보면 반만 열렸어야 하는 거 아닌가? 프로세스별이라서 그런가.


1. `soft` vs `hard` limit
    - soft리밋은 hard리밋을 넘을 수 없고, hard리밋은 줄어들기만 함 <a href="https://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user" target="_blank">(링크)</a>
    - __soft limit__ 이 실제 적용됨
    - `umlimit -a` : soft리밋만 나오는듯
    - `ulimit -Hn` : 하드리밋, `ulimit -Sn` : 소프트 리밋
    - cat /proc/\{procNum\}/limits `proc 파일시스템` 이 제일 자세함
2. hard limit 바꾸기
    - 링크 : <a href="https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F" target="_blank">https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F</a>
    ```
    이를 늘려주기 위해서는
    vim /etc/security/limits.conf
    맨아래에 아래를 추가해준다
    *                soft    nofile          100000
    *                hard    nofile          100000
    *                soft    noproc          100000
    *                hard    noproc          100000
    ulimit -n 40960
    ```
    - 
3. 전체 프로세스의 fd리밋 합은 따로 있음


#### 포트의 개념
- 4레이어에만 있는 거고, 논리적인 번호임. 어플리케이션을 구분하기 위한 것
- <a href="https://memoweb.tistory.com/entry/%ED%8F%AC%ED%8A%B8%EC%9D%98-%EA%B0%9C%EB%85%90%EA%B3%BC-%EB%B3%B8%EC%A7%88-port" target="_blank">참고</a>
- 커널은 소켓을 (serv ip, serv port, cli ip, cli port)의 키로 구분함

#### TCP backlog
- `listen`시에 넣어주는 파라미터
- `SYN`을 받은 서버가 `SYN+ACK`을 보내고 아직 `ACK`를 못받아 established되지 않은 소켓을 몇개까지 유지할거냐 하는 것. 넘 크면 flooding같은걸 당하는듯

#### MISCS
- clinet 포트번호 지정 - `bind()`로 가능 <a href="https://www.geeksforgeeks.org/explicitly-assigning-port-number-client-socket/" target="_blank">https://www.geeksforgeeks.org/explicitly-assigning-port-number-client-socket/</a>. 평소엔 자동으로 지정되고 있었는듯
  - 하긴 클라이언트가 포트번호를 지정하면 오히려 범용성이 좀 떨어질듯 <a href="https://m.blog.naver.com/rev7707" target="_blank">https://m.blog.naver.com/rev7707</a> 이블로그 뭔가 나랑 겹치는게 많음
- `Slow consumer` 이슈 : 로컬 tcp buffer가 꽉 차있지 않아도, 원격 TCP버퍼가 꽉 차 있으면 block이 걸림 (you can think of the remote TCP buffer, the network and the local sending TCP buffer, as one big buffer) <a href="https://stackoverflow.com/questions/11037867/socket-send-call-getting-blocked-for-so-long" target="_blank">https://stackoverflow.com/questions/11037867/socket-send-call-getting-blocked-for-so-long</a>

### thread per socket + blocking 
- 왜이렇게 느린가? 500도 제대로 처리 못함
- 프로세스 1000개는 껌으로 만드는데. 500개가 polling하는 시나리오가 문제가 되는 건가?

#### close 처리
- close처리를 잘 안해줬었음
- tcpdump -> wireshark에서, cli가 serv에 [FIN, ACK]는 보내는데 이 이후 종료 과정이 진행되지 않는 것을 발견
- 서버에서 read시 0이나 음수가 리턴될 시 close를 불러 줘야 하는데 이 처리를 안해서, `close_wait소켓이 계속 유지되고 있었음`
  - 이거 나중에 확인해보자 (netstat -napo로)    
    <img src="{{ site.url }}/images/coding/tcp/close_wait.jpg" width="350" class="center"/>  
    서버에서는 read에서 0 return하면 socket close를 해줘야 하는데 이를 처리 안해서 계속 죽은 소켓에서 읽고 있고 (sigpipe 나지 않나?), 클라이언트에선 FIN에 대한 ACK는 받았으나 서버에서 close호출로 보내오는 FIN을 받지 못해 FIN_WAIT2에 걸림 (timeout 후 소켓 없어짐)
- 이제 TIME_WAIT 대량발생 시험해 볼 수 있을 듯

### C500K
- <a href="https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/" target="_blank">https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/</a>

### Kernel compiles
- 여기에 정리해놓자