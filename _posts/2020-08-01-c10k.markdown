---
layout: post
title:  "TCP socket + C10K Problem"
date:   2020-07-24 09:00:05 +0800
categories: problem_solving
use_math: true
tags: problem_solving need_review coding
---

유용한 링크들
- <a href="https://github.com/nailbrainz/c10k_test" target="_blank">github sources link</a>  
- <a href="https://d2.naver.com/helloworld/47667" target="_blank">TCP-IP의 근본</a>
- <a href="http://docs.likejazz.com/time-wait/" target="_blank">엄청 잘쓴 블로그</a> 여기 코드 좀 참고할 만 한듯
- <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/" target="_blank">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a>
- <a href="https://www.cs.dartmouth.edu/~sergey/netreads/path-of-packet/Network_stack.pdf" target="_blank">PATH OF A PACKET IN THE LINUX KERNEL STACK</a>

계획
1. select 기반 - 해봐야 1000개밖에 안될테지만..
    - 링크
2. epoll
3. 커널 컴파일 - 제로 카피?
4. nginx
5. TODO
   1. <a href="https://gist.github.com/Alexey-N-Chernyshov/4634731" target="_blank">https://gist.github.com/Alexey-N-Chernyshov/4634731</a> : catch sigint, sigterm and terminate gracefully

이슈들
- select최대는 1000개. 컴파일 타임에 변경도 못한다 함
- socket생성 - bind (app과 포트번호 바인딩) - listen (server socket 활성화) 까지는 전부 동일하므로, 그 이후를 함수포인터로 추상화해서 메인의 중복을 줄임



### 공부할 것
- https://recipes4dev.tistory.com/153 TCP IP conn process. must review
- https://m.blog.naver.com/PostView.nhn?blogId=rev7707&logNo=10005157701&proxyReferer=https:%2F%2Fwww.google.com%2F 클라이언트는 포트 번호를 특정한 값으로 지정할 필요가 없다 (클라이언트에서는 보통 시스템이 자동적으로 배정하는 포트 번호를 사용한다.).
- why select is N^2? https://stackoverflow.com/questions/19282784/why-is-poll-better-than-select Many programs use other approaches. For instance sshd, the SSH daemon, spawns a child process for each connection. Others handle each connection in a thread.
- http://www.kegel.com/c10k.html#nb from the basics
- http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html kernel matters
- https://github.com/trptcolin/c10k I like the code
- https://stackoverflow.com/questions/10538291/what-is-the-point-of-noreturn attribute noreturn
- https://github.com/mrkschan/c10k-experiment python c10k
- https://www.slideshare.net/igalarzab/communication-in-python-and-the-c10k-problem comminication in python and c10k problem
- http://youngrok.com/asyncio%EC%97%90%20%EB%8C%80%ED%95%9C%20%ED%9A%8C%EC%9D%98 asyncio에 대한 회의
- https://github.com/aaugustin/django-c10k-demo



### TCP Socket
- 소켓의 식별자 = (source ip, source port, dest ip, dest port)

#### basics
> <a href="https://d2.naver.com/helloworld/47667" target="_blank">https://d2.naver.com/helloworld/47667</a>

- `Flow control` : 송신자는 수신자가 받을 수 있는 만큼 데이터를 전송한다. 수신자가 자신이 받을 수 있는 바이트 수 (사용하지 않은 버퍼 크기, receive window)를 송신자에게 전달한다. 송신자는 수신자 receive window가 허용하는 바이트 수만큼 데이터를 전송한다.
- `Congestion control` : 네트워크 정체를 방지하기 위해 receive window와 별도로 congestion window를 사용하는데 이는 네트워크에 유입되는 데이터양을 제한하기 위해서이다. Receive window와 마찬가지로 congestion window가 허용하는 바이트 수만큼 데이터를 전송하며 여기에는 TCP Vegas, Westwood, BIC, CUBIC 등 다양한 알고리즘이 있다. __Flow control과 달리 송신자가 단독으로 구현한다.__
- 여러 레이어가 있지만, 크게 유저(user) 영역, 커널(kernel) 영역, 디바이스로(device) 영역으로 나눌 수 있다. 유저 영역과 커널 영역에서의 작업은 CPU가 수행한다. __이 유저 영역과 커널 영역은 디바이스 영역과 구별하기 위해 호스트(host)라고 부른다__
- 


establish, closing
- 당연한 말이지만 연결시 클라이언트가 무조건 먼저 걸음 (syn - syn+ack - ack)
- 종료시에는 서버/클라이언트 둘 다 active closer가 될 수 있음 (fin - ack, fin - ack)

#### TIME_WAIT
- active closer측에서 fin을 받고 (4-way에서 3번째) ack를 보낸 뒤 (4번째), 잠시동안 소켓을 유지하는 기간
- <a href="http://docs.likejazz.com/time-wait/" target="_blank">참고 링크</a>
- 왜 있나?
  - 지연된 데이터 패킷 대응 (FIN - ACK, FIN - ACK 정상 종료 후 수신되는 데이터 패킷)
  - active closer가 보내는 마지막 (4-way에서 4th) ack유실시, 상대방 (passive closer)은 `LAST-ACK`상태를 유지하고 (확인 필요) FIN 을 다시 보냄. 이 때 time_wait이 짧으면 active closer는 이미 자체적으로 소켓을 회수한 상태라, 유실된 ACK를 다시 보내주지 않고 RST를 보내게 됨
    - 결국 __Active closer는 자기가 보낸 마지막 ACK가 잘 갔는지 말았는지 모르기 때문에, 일정기간 기다려 줘야 하는 것__

소켓 생성/삭제가 잦은 경우, TIME_WAIT때문에 file descriptor (이건 좀...기본제한이 커서)/로컬 포트가 부족해 질 수 있다 함. ex) 서버는 DB 등에 대해 클라이언트가 되기도
- 클라이언트 : 항상 연결을 거는 입장이므로, 걸기 전 connection pool을 활용 (세션의 cache)
  - `net.ipv4.tcp_tw_reuse` : 로컬 포트 고갈 시 TIME_WAIT상태에 있는 소켓을 빨리 회수해 재사용 가능하게 해 줌
- `서버는 소켓을 열어 놓고 요청을 받는 입장이기 때문에 로컬포트고갈이 일어나지 않는다`
  - 위에도 있다시피, 소켓의 identifier 중 source port가 고정됨. connection은 여러 개 열지만, 다 같은 (source ip, source port) 로 여는 것 
  - `net.ipv4.tcp_tw_recycle` : TIME_WAIT값을 고정된 값이 아니라, RTO기반의 짧은 값으로 바꿔 줌. 
  - 또는 서버측에서 nginx / http keepalive를 세팅하면, GET등의 요청 처리 후 소켓을 잠시 살려 둠

예시
- 웹 --- nginx / 아파치 (웹 서버) --- tomcat (앱 서버) 에서, 웹서버-앱서버 간 `keepalive`를 적용하지 않으면 `TIME_WAIT` 발생
  - 소켓 고갈 (클라이언트=웹서버)은 tw_reuse로 어느정도 해결가능
  - 불필요한 3,4 way handshakes - 성능 낭비 발생
  - 웹 서버를 두는 이유 = HTTPS를 사용시, 인증서 설정/관리, <a href="https://www.popit.kr/%EB%82%B4-%EC%84%9C%EB%B2%84%EC%97%90%EB%8A%94-%EB%88%84%EA%B0%80-%EB%93%A4%EC%96%B4%EC%98%A4%EB%8A%94%EA%B1%B8%EA%B9%8C-%EC%8B%A4%EC%8B%9C%EA%B0%84-user-agent-%EB%B6%84%EC%84%9D%EA%B8%B0/" target="_blank">UserAgent</a> 확인, <a href="https://damedame.tistory.com/entry/HTTP-referer" target="_blank">HTTP Referer확인</a> 등 서비스 외적인 요소가 많기 때문에, __서비스 외적 (웹서버) / 서비스 (앱서버)__ 분리용
    - <a href="https://ko.wikipedia.org/wiki/HTTP_%EB%A6%AC%ED%8D%BC%EB%9F%AC" target="_blank">위키 리퍼러 페이지</a>


#### MISC
- `sudo sysctl -w net.core.somaxconn="2048"` : 무슨 효과가 있는거지? 엄청 낮게 설정해도 별거 없음
- 

### 파일 디스크립터 갯수 늘리기
내 C10K 첫번째 이슈였음. soft limit이 1024여서 930개 정도까지밖에 안열림. 근데 생각해보면 반만 열렸어야 하는 거 아닌가? 프로세스별이라서 그런가.


1. `soft` vs `hard` limit
    - soft리밋은 hard리밋을 넘을 수 없고, hard리밋은 줄어들기만 함 <a href="https://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user" target="_blank">(링크)</a>
    - __soft limit__ 이 실제 적용됨
    - `umlimit -a` : soft리밋만 나오는듯
    - `ulimit -Hn` : 하드리밋, `ulimit -Sn` : 소프트 리밋
    - cat /proc/\{procNum\}/limits `proc 파일시스템` 이 제일 자세함
2. hard limit 바꾸기
    - 링크 : <a href="https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F" target="_blank">https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F</a>
    ```
    이를 늘려주기 위해서는
    vim /etc/security/limits.conf
    맨아래에 아래를 추가해준다
    *                soft    nofile          100000
    *                hard    nofile          100000
    *                soft    noproc          100000
    *                hard    noproc          100000
    ulimit -n 40960
    ```
    - 
3. 전체 프로세스의 fd리밋 합은 따로 있음


#### 포트의 개념
- 4레이어에만 있는 거고, 논리적인 번호임. 어플리케이션을 구분하기 위한 것
- <a href="https://memoweb.tistory.com/entry/%ED%8F%AC%ED%8A%B8%EC%9D%98-%EA%B0%9C%EB%85%90%EA%B3%BC-%EB%B3%B8%EC%A7%88-port" target="_blank">참고</a>
- 커널은 소켓을 (serv ip, serv port, cli ip, cli port)의 키로 구분함

#### TCP backlog
- `listen`시에 넣어주는 파라미터
- `SYN`을 받은 서버가 `SYN+ACK`을 보내고 아직 `ACK`를 못받아 established되지 않은 소켓을 몇개까지 유지할거냐 하는 것. 넘 크면 flooding같은걸 당하는듯

#### MISCS
- clinet 포트번호 지정 - `bind()`로 가능 <a href="https://www.geeksforgeeks.org/explicitly-assigning-port-number-client-socket/" target="_blank">https://www.geeksforgeeks.org/explicitly-assigning-port-number-client-socket/</a>. 평소엔 자동으로 지정되고 있었는듯
  - 하긴 클라이언트가 포트번호를 지정하면 오히려 범용성이 좀 떨어질듯 <a href="https://m.blog.naver.com/rev7707" target="_blank">https://m.blog.naver.com/rev7707</a> 이블로그 뭔가 나랑 겹치는게 많음
- `Slow consumer` 이슈 : 로컬 tcp buffer가 꽉 차있지 않아도, 원격 TCP버퍼가 꽉 차 있으면 block이 걸림 (you can think of the remote TCP buffer, the network and the local sending TCP buffer, as one big buffer) <a href="https://stackoverflow.com/questions/11037867/socket-send-call-getting-blocked-for-so-long" target="_blank">https://stackoverflow.com/questions/11037867/socket-send-call-getting-blocked-for-so-long</a>

### thread per socket + blocking 
- 왜이렇게 느린가? 500도 제대로 처리 못함
- 프로세스 1000개는 껌으로 만드는데. 500개가 polling하는 시나리오가 문제가 되는 건가?


### C500K
- <a href="https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/" target="_blank">https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/</a>

### Kernel compiles
- 여기에 정리해놓자