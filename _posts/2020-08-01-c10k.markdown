---
layout: post
title:  "C10K Problem"
date:   2020-08-01 09:00:05 +0800
categories: coding
use_math: true
tags: os need_review coding
---

https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux

09.01 실험 정리
1. 구린컴(클라이언트) -> 신컴(서버)로, 90mb/s밖에 데이터가 안 옴. 내생각엔 구린컴 한계인 듯
2. 클라/서버를 반대로 하면 구린컴이 느려서 터짐
3. 신컴(클라) -> 신컴(서버) 로 하기로 함. 클라이언트 개선 (10000개 소켓 - 4개에 프로세스에 나눠서, 랜덤으로 데이터 전송)했는데, 로컬호스트 (NIC) 에 걸리는 로드가 초당300mb이 한계인 듯. 이걸로는 서버의 load_avg가 1이하임
4. 결국 클라이언트를 구리게 하면, 멀티쓰레드일시는 멈추고 epoll은 빠름. difference를 찾았으니 그만할까..
   1. 근데 또, epoll에서는 클라이언트를 10000개로 하면 packet drop (sigpipe) 이 발생. 1 process epoll으로는 처리불가능해서 그런 듯. 2thread로 돌려도 되겠지만 그냥 클라갯수를 5000개로 줄임
      1. 스택오버플로우에선 sigpipe를 무시해야 한다고 하네? <a href="https://stackoverflow.com/questions/108183/how-to-prevent-sigpipes-or-handle-them-properly" target="_blank">링크</a>
   2. 라즈베리 파이/비글본을 서버로 해도 되겠지만...귀찮아서....일단 여기서 끝내자. nginx 해보는 게 더 도움이 될 듯
   3. 이 종료상황 잘 이해가 안되는데 함 보자 https://stackoverflow.com/questions/33053507/econnreset-in-send-linux-c


### 파일디스크립터 리밋 변경하기
- https://qastack.kr/ubuntu/1049058/how-to-increase-max-open-files-limit-on-ubuntu-18-04
- /etc/systemd/system.conf, /etc/systemd/user.conf 에서 DefaultLimitNOFILE 찾아서 주석해제하고 값 입력 (프로세스 수도 동일하게 변경가능)
- "ulimit -n 65536" 해줘야 할 수도 있음
- /etc/security/limits.conf 여기는 안바꿔도 되는 것 같은데 잘 모르겠음

유용한 링크들
- <a href="https://github.com/nailbrainz/c10k_test" target="_blank">github sources link</a>  
- <a href="http://docs.likejazz.com/time-wait/" target="_blank">엄청 잘쓴 블로그</a> 여기 코드 좀 참고할 만 한듯
- <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/" target="_blank">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a>
- <a href="https://www.cs.dartmouth.edu/~sergey/netreads/path-of-packet/Network_stack.pdf" target="_blank">PATH OF A PACKET IN THE LINUX KERNEL STACK</a> <a href="https://pigbrain.github.io/network/2016/05/29/PathOfAPacketInTheLinuxKernelStack_on_Network" target="_blank">(한국어)</a>
- <a href="https://www.oreilly.com/library/view/linux-device-drivers/0596000081/ch05s02.html" target="_blank">blocking read, kernel side</a>
- <a href="https://evan-moon.github.io/2019/11/22/tcp-flow-control-error-control/" target="_blank">컨제스쳔 컨트롤, 에러컨트롤 대박 사이트</a>
- <a href="https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/" target="_blank">history and brokeness of select, epoll</a>
- <a href="https://uwsgi-docs.readthedocs.io/en/latest/articles/SerializingAccept.html" target="_blank">Thundering herds problem</a>
  - <a href="https://lwn.net/Articles/632590/" target="_blank">epoll thundering herd patch</a>

implementation of epoll
- <a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/" target="_blank">select, epoll의 구현. 이거만 잘 읽자</a>
  - `fs/eventpoll.c`
  - `net/ipv4/tcp.c`
  - 결국, socket filesystem에 `wait_queue`를 하나 등록시키고, 여기서 이벤트 기반으로 변화가 있을시마다 콜백함수가 호출되어 epoll구조체에 정보를 전닳해 줌. wait_queue기반의 이벤트라는것이 중요한 듯. 아직 polling 공부를 못해서...
- https://programming.vip/docs/linux-kernel-notes-epoll-implementation-principle.html
- optional : https://fd3kyt.github.io/posts/implementation-of-epoll/#fnref:fn-3
- https://www.slideshare.net/llj098/epoll-from-the-kernel-side

위의 것 prerequisite : wait_queue
1. <a href="https://www.kernel.org/doc/htmldocs/kernel-hacking/queues.html" target="_blank">커널 doc 설명</a>
   1. 파일시스템을 예로 들면, 각 파일마다 wait해야 하는 이벤트가 있다고 하자. 그럼 각 fd마다 `wait_queue_head_t`를 우선 선언함
   2. 이 파일시스템을 이용하는 proc중, 이 이벤트를 기다리고 싶은 프로세스는, `wait_queue_t` 를 선언하여 헤드에 붙이고 자신의 실행을 양보해서 sleep에 들어감 (인터럽티블 - `wait_event_interruptible`, 언인터럽티블 - `wait_event` 함수를 이용)
   3. 특정 조건이 만족되면, 다른 프로세스가 `wake_up()`을 호출함 - `TASK_EXCLUSIVE` 가 세팅되어 있으면 1개, 아니면 대기 큐에 있는 모든 프로세스가 TASK_RUNNING으로 전환되어 스케쥴링에 참여

States  
<img src="https://upload.wikimedia.org/wikipedia/en/5/57/Tcp_state_diagram.png" width="600" class="center"/>  


계획
1. select 기반 - 해봐야 1000개밖에 안될테지만..
    - 링크
2. epoll
3. 커널 컴파일 - 제로 카피?
4. nginx
5. TODO
   1. <a href="https://gist.github.com/Alexey-N-Chernyshov/4634731" target="_blank">https://gist.github.com/Alexey-N-Chernyshov/4634731</a> : catch sigint, sigterm and terminate gracefully

이슈들
- select최대는 1000개 컴파일 타임에 변경도 못한다 함
- socket생성 - bind (app과 포트번호 바인딩) - listen (server socket 활성화) 까지는 전부 동일하므로, 그 이후를 함수포인터로 추상화해서 메인의 중복을 줄임



### 공부할 것
- https://recipes4dev.tistory.com/153 TCP IP conn process. must review
- https://m.blog.naver.com/PostView.nhn?blogId=rev7707&logNo=10005157701&proxyReferer=https:%2F%2Fwww.google.com%2F 클라이언트는 포트 번호를 특정한 값으로 지정할 필요가 없다 (클라이언트에서는 보통 시스템이 자동적으로 배정하는 포트 번호를 사용한다.).
- why select is N^2? https://stackoverflow.com/questions/19282784/why-is-poll-better-than-select Many programs use other approaches. For instance sshd, the SSH daemon, spawns a child process for each connection. Others handle each connection in a thread.
- http://www.kegel.com/c10k.html#nb from the basics
- http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html kernel matters
- https://github.com/trptcolin/c10k I like the code
- https://stackoverflow.com/questions/10538291/what-is-the-point-of-noreturn attribute noreturn
- https://github.com/mrkschan/c10k-experiment python c10k
- https://www.slideshare.net/igalarzab/communication-in-python-and-the-c10k-problem comminication in python and c10k problem
- http://youngrok.com/asyncio%EC%97%90%20%EB%8C%80%ED%95%9C%20%ED%9A%8C%EC%9D%98 asyncio에 대한 회의
- https://github.com/aaugustin/django-c10k-demo



#### MISC
- `sudo sysctl -w net.core.somaxconn="2048"` : 무슨 효과가 있는거지? 엄청 낮게 설정해도 별거 없음
- 

### 파일 디스크립터 갯수 늘리기
내 C10K 첫번째 이슈였음. soft limit이 1024여서 930개 정도까지밖에 안열림. 근데 생각해보면 반만 열렸어야 하는 거 아닌가? 프로세스별이라서 그런가.


1. `soft` vs `hard` limit
    - soft리밋은 hard리밋을 넘을 수 없고, hard리밋은 줄어들기만 함 <a href="https://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user" target="_blank">(링크)</a>
    - __soft limit__ 이 실제 적용됨
    - `umlimit -a` : soft리밋만 나오는듯
    - `ulimit -Hn` : 하드리밋, `ulimit -Sn` : 소프트 리밋
    - cat /proc/\{procNum\}/limits `proc 파일시스템` 이 제일 자세함
2. hard limit 바꾸기
    - 링크 : <a href="https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F" target="_blank">https://m.blog.naver.com/PostView.nhn?blogId=gura2013&logNo=80206126707&proxyReferer=https:%2F%2Fwww.google.com%2F</a>
    ```
    이를 늘려주기 위해서는
    vim /etc/security/limits.conf
    맨아래에 아래를 추가해준다
    *                soft    nofile          100000
    *                hard    nofile          100000
    *                soft    noproc          100000
    *                hard    noproc          100000
    ulimit -n 40960
    ```
    - 
3. 전체 프로세스의 fd리밋 합은 따로 있음



### C500K
- <a href="https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/" target="_blank">https://threadbuilder.wordpress.com/2013/06/20/linux-kernel-tuning-for-c500k/</a>

### Kernel compiles
- 여기에 정리해놓자


### select

<a href="http://www.kegel.com/c10k.html" target="_blank">The C10K Problem</a>

(e)poll vs select
- <a href="https://daniel.haxx.se/docs/poll-vs-select.html" target="_blank">https://daniel.haxx.se/docs/poll-vs-select.html</a>
    - poll이랑 select는 비슷하고, epoll은 event driven?
- <a href="https://ozt88.tistory.com/21" target="_blank">https://ozt88.tistory.com/21</a> 
    > epoll은 select의 단점을 많이 개선한 형태의 통지방식이다. FD_SET을 운영체제가 직접 관리하는 것으로 많은 부분이 개선되었다. 하지만 그 본질적인 동작 구조는 select와 크게 다르지 않다. 프로세스가 커널에게 지속적으로 I/O 상황을 체크하여 동기화 하는 개념은 여전히 유효하다. 따라서 epoll의 통지모델 역시 동기형 통지모델이다.   
    그리고 timeout개념이 select와 동일한 방식으로 동작하기 때문에 timeout에 들어온 인자가 어떠냐에 따라 blocking이기도 하고 non-blocking이기도 하다. 따라서 epoll의 전체적인 개념모델은 select와 같다고 생각한다.
- <a href="https://d2.naver.com/helloworld/1469717" target="_blank">네이버 D2 - thread vs epoll</a>



출처: https://ozt88.tistory.com/21 [공부 모음]

<a href="https://github.com/nailbrainz/c10k_test/blob/master/src/c10k_select.c" target="_blank">c10k select example</a>

### controls
```cpp
fd_set readfds;
FD_ZERO(&readfds);
FD_SET(0, &readfds);  //readfds의 0번 비트를 1로
FD_SET(socketfd, &readfds); //readfds의 sockfd번 비트를 1로
FD_CLR(i, &reads); //readfds의 i 비트를 0으로
if(FD_ISSET(i, &cpy_reads)); //cpy_reads i 비트가 1인가?

sock_fd = accept(serv_fd, (struct sockaddr *)&client_addr, &add_struct_size);
FD_SET(sock_fd, &reads); //새로 할당하기
```


### epoll