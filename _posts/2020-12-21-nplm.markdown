---
layout: post
title:  "A Neural Probabilistic Language Model"
date:   2020-12-21 08:00:05 +0800
categories: math deep_learning
use_math: true
tags: math need_review deep_learning recomm
---

- <a href="https://jmlr.org/papers/volume3/tmp/bengio03a.pdf" target="_blank">https://jmlr.org/papers/volume3/tmp/bengio03a.pdf</a>
  -  <a href="https://papers.nips.cc/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf" target="_blank">https://papers.nips.cc/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf</a>
- <a href="http://papers.neurips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning" target="_blank">http://papers.neurips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning</a> 뭐지 재밌어 보이는데



### Abstract
- __curse of dimensionality__ : a word sequence on which the model will be tested is likely to be different from all the word seqs seen during training
  - traditionally : `n-gram` obtain generalization by concatenating very short overlapping seqs seen in the training set
  - we propose __learning a distributed repr for words__ (=embedding!)
- the model learns embedding + model( prob func for word sequences) simultaneously

> Generalization is obtained b/c a seq of words that has never been seen before gets high prob if it is made of words that are similar to words forming an already seen sentence

ex) Let a seq "a cat eats feed" be in a training set. If we have (good) embedding which assings similar (distributed) expr to words "cat" and "dog", then we may have good infer result on the unseen "a dog eats feed" seq.


- sequence of words : 문장
- distributed representation : word embedding


### Intro

100,000 words, model with 10 conseq words  
\\(\rightarrow 100000^{10}=10^{50}\\) free params

We attempt generalization by
- continuous model : smooth assumption
- discrete : __not so obvious__ (number repr = hamming distance gives unintentional results)

In `non-parametric (kernel) density estimation` <a href="https://darkpgmr.tistory.com/147" target="_blank">(link)</a>
- in high dimension, it is crucial to distribute prob mass __where it matters,__ rather than uniformly in all directions around each training point

A `statistical model of language`
\\[\hat{P}(W\_1^T) = \prod\_{t=1}^T \hat{P}(w\_t\|W\_1^{t-1})\\]
where
- \\(w\_t\\) : \\(t\\)-th word
- sub-sequence \\(w\_i^j = (w\_i,w\_{i+1},...,w\_{j-1},w\_j)\\)

#### N-gram
`n-gram` model construct tables of conditional probabilities for the next word, for each one of a large # of `context`, i.e. combination of the last \\(n-1\\) words 
\\[\hat{P}(W\_t \| w\_1^{t-1})\approx \hat{P}(W\_t \| w\_{t-n+1}^{t-1})\\]

Generalization in `n-gram` models
- look at the prob predicted using a __smaller context size__ (smaller than \\(n\\)), as done in `back-off trigram` or `smoothed(interpolated) trigram`
- (similarly in) __generative__ model, a new (unseen) seq of words is generated by __gluing__ very short and __overlapping__ pieces of length 1, 2, ... that have been seen frequently in the training data
- \\(n=3\\) gram showed empirical superiority (don't know why)

However such models
- are restrictred to consider shorter context (\\(n\leq 3\\))
- canot assign similar score to similar words


### A Neural Models (in 2003)

We decompose the function \\(f(w\_t,...,w\_{t-n+1})=\hat{P}(w\_t\|w\_1^{t-1})\\) in two parts
1. A mapping (=embedding) \\(C\in \Re^{\|V\|\times m}\\), where
   - \\(V\\) : word dictionary
   - \\(m\\) : embedding length. 30, 50, 100,...
2. The probability function \\(g\\) over words, which uses embedding \\(C\\) as input
\\[f(w\_t,...,w\_{t-n+1})=\hat{P}(w\_t\|w\_1^{t-1}) = g(i,C(w\_{t-1},...,C(w\_{t-n+1})))\\]


<img src="{{site.url}}/images/deeplearning/nlp/bengio/model.jpg" width="900">  

The final output probability is calculated using softmax of the unnormalized log probs \\(y\\)
\\[\hat{P}(w\_t\|w\_{t-1},...,w\_{t-n+1}) = \frac{e^{y\_{w\_t}}}{\sum\_i y\_{w\_i}}\\]
where \\(y\\) is defined as
\\[y = b + Wx + u\text{tanh}(d+Hx)\\]
here
- \\(W\\) is optimally zero
- \\(x\\) is the word embedding, which is the concat of the input word features from embedding \\(C\\)
\\[x := (C(w\_{t-1}),C(w\_{t-2}),...,C(w\_{t-n+1}))\\]

The function \\(f\\) is a composition of these two mappings \\(C, G\\), with \\(C\\) __being shared across all the words in the context.__
- the training is achieved by looking for param \\(\theta=(C,\omega)\\) that maximizes the training corpus `penalized (L2) log-likelihood`
- the # of free params __only scales linearly__, wrt \\(V\\) and \\(n\\) (length of context we consider)


### Parallel computation
- used CPU and IPC?

### Experiments
- 3-splits
  - `train` = .7 training
  - `validation` = .15 model selection, weight decay, early stopping
  - `test` = .15 testing
- 47578 words - Rare words with freq < 3  were merged into a single symbol, reducing the \\(\|V\|\\) to 16383

#### Interpolated, Smoothed Trigram
- Jelinek and Mercer, 1980(!)
- Let \\(q\_t=l(freq(w\_{t-1},w\_{t-2}))\\) represents the discretized freq of occurence of the input context \\((w\_{t-1},w\_{t-2})\\)
- Then the conditional prob estimates have the form of a conditional mixture
\\[\hat{P}(w\_t\|w\_{t-1},w\_{t-2}) = \alpha\_0(q\_t)p\_0 + \alpha\_1(q\_t)p\_1(w\_t) + \alpha\_2(q\_t)p\_2(w\_t\|w\_{t-1}) + \alpha\_3(q\_t)p\_3(w\_t\|w\_{t-1},w\_{t-2})  \\]

we have
- conditional weights \\(\alpha\_i(q\_t) \geq 0, \sum\_i\alpha\_i(q\_t) = 1\\) for each context \\(w\_{t-1},w\_{t-2}\\).
- The base predictors are the following:
  - \\(p\_0\\) : \\(1/\|V\|\\) (base prob)
  - \\(p\_1(i) \\) is the `unigram` (relative freq of word \\(i\\) in the training set)
  - \\(p\_2(i\|j) \\) is the `bigram` (relative freq of word \\(i\\) when prev word was \\(j\\), in the training set)
  - \\(p\_3(i\|j,k) \\) is the `trigram` (relative freq of word \\(i\\) when prev word was \\(j,k\\), in the training set)
  - The motivation is that we are indeed __gluing__ very short and __overlapping__ pieces, and want to give more weight to freq pieces
- used as a mixture with the MLP, since they appear to make errors in very different ways


#### Main results
- neural network better
- mixing the output probs of the neural net with the interpolated trigram always helps to reduce perplexity

__out of vocal words__
- Let a word \\(j\notin V\\) is observed, in fairly known context \\(w\_{t-n+1}^{t-1}\\). We initialize \\(C(j)\\) as
\\[C(j) \leftarrow \sum\_{i\in V}C(i) + \hat{P}(i\|w\_{t-n+1}^{t-1})\\]
and renormalize, which is a convex combination based on occurence within the observed context.


Other future works
- progagating gradients only from a subset of the output words?
- introducing a-priori knowledge (pre-training, bayesian?)
- __use a time-deelay and possibly recurrent neural networks__

### Conclusion
> We believe that the main reason for these improvements is that the proposed approach allows to take advantage of the learned distributed representation to fight the curse of dimensionality with its own weapons: each training sentence informs the model about a combinatorial number of other sentences.

- More generally, the work presented here opens the door to improvements in statistical language models brought by replacing “tables of conditional probabilities” by more compact and smoother representations based on distributed representations that can accommodate far more conditioning variables.
- Whereas much effort has been spent in statistical language models (e.g. stochastic grammars) to restrict or summarize the conditioning variables in order to avoid overfitting, the type of models described here shifts the difficulty elsewhere: many more computations are required, but computation and memory requirements scale linearly, not exponentially with the number of conditioning variables.